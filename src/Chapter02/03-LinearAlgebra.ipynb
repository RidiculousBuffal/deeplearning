{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 标量",
   "id": "d23ef7a0883c9c77"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:03.667898Z",
     "start_time": "2025-07-23T13:32:59.949657Z"
    }
   },
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x+y,x*y,x/y,x**y"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 向量\n",
    "$$\n",
    "\\mathbf{x}=\\left[ \\begin{array}{c}\n",
    "\tx_1\\\\\n",
    "\tx_2\\\\\n",
    "\t\\vdots\\\\\n",
    "\tx_n\\\\\n",
    "\\end{array} \\right] \n",
    "$$\n"
   ],
   "id": "ed32d459cebd3865"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:04.129405Z",
     "start_time": "2025-07-23T13:33:04.119408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ],
   "id": "20771a489727f956",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:04.192096Z",
     "start_time": "2025-07-23T13:33:04.184092Z"
    }
   },
   "cell_type": "code",
   "source": "x[3]",
   "id": "5b746dcaa8c1c53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:04.337071Z",
     "start_time": "2025-07-23T13:33:04.331900Z"
    }
   },
   "cell_type": "code",
   "source": "len(x)",
   "id": "2e2b23c6085a65e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> 当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度。形状（shape）是一\n",
    "个元素组，列出了张量沿每个轴的长度（维数）。对于只有一个轴的张量，形状只有一个元素。"
   ],
   "id": "fdc0425cb6e5068c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:04.459598Z",
     "start_time": "2025-07-23T13:33:04.455574Z"
    }
   },
   "cell_type": "code",
   "source": "x.shape",
   "id": "ec458be0fb97403d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 矩阵\n",
    "$$\n",
    "\\boldsymbol{A}_{m\\times n}=\\left[ \\begin{array}{l}\n",
    "\ta_{11}&\t\ta_{12}&\t\t\\cdots&\t\ta_{1n}\\\\\n",
    "\ta_{21}&\t\ta_{22}&\t\t\\cdots&\t\ta_{2n}\\\\\n",
    "\t\\vdots&\t\t\\vdots&\t\t\\ddots&\t\t\\vdots\\\\\n",
    "\ta_{m1}&\t\ta_{m2}&\t\t\\cdots&\t\ta_{mn}\\\\\n",
    "\\end{array} \\right] \n",
    "$$\n"
   ],
   "id": "546a1956e538674e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:04.524977Z",
     "start_time": "2025-07-23T13:33:04.521469Z"
    }
   },
   "cell_type": "code",
   "source": "A = torch.arange(20).reshape(5,4)",
   "id": "e4c41ea7846d06d2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:04.564387Z",
     "start_time": "2025-07-23T13:33:04.559479Z"
    }
   },
   "cell_type": "code",
   "source": "A",
   "id": "fadb8e48e743d07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:05.770274Z",
     "start_time": "2025-07-23T13:33:05.763270Z"
    }
   },
   "cell_type": "code",
   "source": "A.T",
   "id": "cf1f6f3f45795164",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:05.879981Z",
     "start_time": "2025-07-23T13:33:05.874510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B == B.T"
   ],
   "id": "31748ffe80d46f9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 张量\n",
    "就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的$n$维数组的通用方法。例如，向量是一阶张量，矩阵是二阶张量。张量用特殊字体的大写字母表示（例如，$\\mathbf{X}$、$\\mathbf{V}$和$\\mathbf{Z}$），它们的索引机制（例如$x_{ijk}$和$[\\mathbf{X}]_{1,2i-1,3}$）与矩阵类似。\n",
    "\n",
    "当我们开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道（channel）轴，用于表示颜色通道（红色、绿色和蓝色）。现在先将高阶张量暂放一边，而是专注于学习其基础知识。"
   ],
   "id": "a656e102ea198851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:06.023092Z",
     "start_time": "2025-07-23T13:33:06.017905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X"
   ],
   "id": "379d49f11b7b35b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 张量算法的基本性质\n",
    "> 标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。"
   ],
   "id": "bd00ff1d74420bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:06.189764Z",
     "start_time": "2025-07-23T13:33:06.182559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = torch.arange(20,dtype=torch.float32).reshape(5,4)\n",
    "B = A.clone()\n",
    "A,A+B"
   ],
   "id": "90a7c0ea356ed5d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "矩阵的乘法是Hadamard积:\n",
    "$$\n",
    "\\mathbf{A}\\odot \\mathbf{B}=\\left[ \\begin{matrix}\n",
    "\ta_{11}b_{11}&\t\ta_{12}b_{12}&\t\t\\dots&\t\ta_{1n}b_{1n}\\\\\n",
    "\ta_{21}b_{21}&\t\ta_{22}b_{22}&\t\t\\dots&\t\ta_{2n}b_{2n}\\\\\n",
    "\t\\vdots&\t\t\\vdots&\t\t\\ddots&\t\t\\vdots\\\\\n",
    "\ta_{m1}b_{m1}&\t\ta_{m2}b_{m2}&\t\t\\dots&\t\ta_{mn}b_{mn}\\\\\n",
    "\\end{matrix} \\right] .\n",
    "$$\n"
   ],
   "id": "b60068b4f591743b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:06.509655Z",
     "start_time": "2025-07-23T13:33:06.503515Z"
    }
   },
   "cell_type": "code",
   "source": "A*B",
   "id": "6eb1fd2aa9bf5859",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘",
   "id": "e1a391f9955f979e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:06.839610Z",
     "start_time": "2025-07-23T13:33:06.833382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ],
   "id": "138cb27823c731da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 降维\n",
    "计算向量中元素的和:\n",
    "$$\n",
    "sum=\\sum_{i=1}^d{x_i}\n",
    "$$\n"
   ],
   "id": "1d3416ad7fbe5f33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:06.992040Z",
     "start_time": "2025-07-23T13:33:06.984599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "x,x.sum()"
   ],
   "id": "ea71c3d7c757a743",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "矩阵中的张量和\n",
    "$$\n",
    "sum=\\sum_{i=1}^m{\\sum_{j=1}^n{a_{ij}}}\n",
    "$$\n"
   ],
   "id": "a41b23b9472b74b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:07.212861Z",
     "start_time": "2025-07-23T13:33:07.206650Z"
    }
   },
   "cell_type": "code",
   "source": "A",
   "id": "832b7fffb7ff48ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。我们还可以指定张量沿哪一个轴来通过求和降低维度。以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。",
   "id": "136611bf1e23dd2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:07.371836Z",
     "start_time": "2025-07-23T13:33:07.366203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0"
   ],
   "id": "1084c7b2ba3b6c2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40., 45., 50., 55.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:07.582011Z",
     "start_time": "2025-07-23T13:33:07.575021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 我们熟悉的三维张量\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print(\"原始张量 X (形状: {}):\".format(X.shape))\n",
    "print(X)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 我们要研究的操作\n",
    "result = X.sum(axis=[0, 1])\n",
    "\n",
    "print(\"X.sum(axis=[0, 1]) 的结果 (形状: {}):\".format(result.shape))\n",
    "print(result)"
   ],
   "id": "1a30c8149659abb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量 X (形状: torch.Size([2, 3, 4])):\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "==================================================\n",
      "\n",
      "X.sum(axis=[0, 1]) 的结果 (形状: torch.Size([4])):\n",
      "tensor([60, 66, 72, 78])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:07.707663Z",
     "start_time": "2025-07-23T13:33:07.702539Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "61a917f4586eff3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:07.902642Z",
     "start_time": "2025-07-23T13:33:07.896181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = X.sum(axis=0)\n",
    "print(t)\n",
    "q = t.sum(axis=0)\n",
    "print(q)"
   ],
   "id": "c6ef3cd3cc15f8d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 14, 16, 18],\n",
      "        [20, 22, 24, 26],\n",
      "        [28, 30, 32, 34]])\n",
      "tensor([60, 66, 72, 78])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:08.023061Z",
     "start_time": "2025-07-23T13:33:08.017171Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "292615079c46c272",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:08.134807Z",
     "start_time": "2025-07-23T13:33:08.129501Z"
    }
   },
   "cell_type": "code",
   "source": "X.sum(axis=[0,2])",
   "id": "607e5d63a0b255b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 60,  92, 124])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:08.247700Z",
     "start_time": "2025-07-23T13:33:08.241808Z"
    }
   },
   "cell_type": "code",
   "source": "X.sum(axis=[1])",
   "id": "69339ae1d5aeccd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 15, 18, 21],\n",
       "        [48, 51, 54, 57]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:08.361302Z",
     "start_time": "2025-07-23T13:33:08.354297Z"
    }
   },
   "cell_type": "code",
   "source": "A",
   "id": "89a36834006e4dbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "求平均",
   "id": "f8d58582fc317e8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:08.572834Z",
     "start_time": "2025-07-23T13:33:08.565836Z"
    }
   },
   "cell_type": "code",
   "source": "A.mean(),A.sum()/A.numel()",
   "id": "874ff426b4b7e812",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:33:08.741998Z",
     "start_time": "2025-07-23T13:33:08.734619Z"
    }
   },
   "cell_type": "code",
   "source": "A.mean(axis=0), A.sum(axis=0) / A.shape[0]",
   "id": "6f000b554ba7156d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "非降维求和 \\\n",
    "但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。"
   ],
   "id": "61220c7e8c8d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:37:25.252515Z",
     "start_time": "2025-07-23T13:37:25.247197Z"
    }
   },
   "cell_type": "code",
   "source": "A,A.shape",
   "id": "9059f081e76c8f49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:35:59.113109Z",
     "start_time": "2025-07-23T13:35:59.107107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sum_A = A.sum(axis=1,keepdim=True)\n",
    "sum_A"
   ],
   "id": "70693e8713cc6fa6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:36:10.196937Z",
     "start_time": "2025-07-23T13:36:10.192100Z"
    }
   },
   "cell_type": "code",
   "source": "A.sum(axis=1)",
   "id": "ee3a9a7a22d6a743",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38., 54., 70.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:51:19.749068Z",
     "start_time": "2025-07-23T13:51:19.744259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "B=torch.arange(24).reshape(4,3,2)\n",
    "B"
   ],
   "id": "a9c5293957543cba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]],\n",
       "\n",
       "        [[18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:55:47.475616Z",
     "start_time": "2025-07-23T13:55:47.470520Z"
    }
   },
   "cell_type": "code",
   "source": "B.sum(axis=2)",
   "id": "88f96a603fed7f5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5,  9],\n",
       "        [13, 17, 21],\n",
       "        [25, 29, 33],\n",
       "        [37, 41, 45]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T13:57:53.412197Z",
     "start_time": "2025-07-23T13:57:53.407457Z"
    }
   },
   "cell_type": "code",
   "source": "B.sum(axis=2,keepdim=True)",
   "id": "e817044d738edb35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1],\n",
       "         [ 5],\n",
       "         [ 9]],\n",
       "\n",
       "        [[13],\n",
       "         [17],\n",
       "         [21]],\n",
       "\n",
       "        [[25],\n",
       "         [29],\n",
       "         [33]],\n",
       "\n",
       "        [[37],\n",
       "         [41],\n",
       "         [45]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:13:37.199298Z",
     "start_time": "2025-07-23T14:13:37.193994Z"
    }
   },
   "cell_type": "code",
   "source": "B.sum(axis=1),B.sum(axis=1,keepdim=True)",
   "id": "9a8ae51928f3ec2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6,  9],\n",
       "         [24, 27],\n",
       "         [42, 45],\n",
       "         [60, 63]]),\n",
       " tensor([[[ 6,  9]],\n",
       " \n",
       "         [[24, 27]],\n",
       " \n",
       "         [[42, 45]],\n",
       " \n",
       "         [[60, 63]]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:07:27.001499Z",
     "start_time": "2025-07-23T14:07:26.997090Z"
    }
   },
   "cell_type": "code",
   "source": "B[0]",
   "id": "3e3c63459a97fa68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:07:45.447549Z",
     "start_time": "2025-07-23T14:07:45.441979Z"
    }
   },
   "cell_type": "code",
   "source": "B[0,0]",
   "id": "4c015d6bcc82234a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:17:16.843455Z",
     "start_time": "2025-07-23T14:17:16.838796Z"
    }
   },
   "cell_type": "code",
   "source": "B.sum(axis=0)",
   "id": "56ed6022f1903275",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36, 40],\n",
       "        [44, 48],\n",
       "        [52, 56]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:18:02.485822Z",
     "start_time": "2025-07-23T14:18:02.480761Z"
    }
   },
   "cell_type": "code",
   "source": "B.sum(axis=0,keepdim=True)",
   "id": "bb35f75335ca5737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[36, 40],\n",
       "         [44, 48],\n",
       "         [52, 56]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果我们想沿某个轴计算A元素的累积总和，比如`axis=0`（按行计算），可以调用`cumsum`函数。此函数不会沿任何轴降低输入张量的维度",
   "id": "793bee8354a306e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:19:40.893470Z",
     "start_time": "2025-07-23T14:19:40.888148Z"
    }
   },
   "cell_type": "code",
   "source": "A.cumsum(axis=0)",
   "id": "33a94d6b0784738a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:21:05.784360Z",
     "start_time": "2025-07-23T14:21:05.779363Z"
    }
   },
   "cell_type": "code",
   "source": "B",
   "id": "50012b7d26d6dbf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]],\n",
       "\n",
       "        [[18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 点积\n",
    "给定两个向量$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$，它们的点积 (dot product) $\\mathbf{x}^\\top\\mathbf{y}$ (或$\\langle\\mathbf{x}, \\mathbf{y}\\rangle$) 是相同位置的按元素乘积的和：\n",
    "$$\\mathbf{x}^\\top\\mathbf{y} = \\sum_{i=1}^d x_i y_i$$。"
   ],
   "id": "506bc1aed289e24a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:06.446713Z",
     "start_time": "2025-07-23T14:28:06.440715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(4,dtype=torch.float32)\n",
    "x"
   ],
   "id": "3723357fb77e8299",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:08.041928Z",
     "start_time": "2025-07-23T14:28:08.036965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = torch.ones(4,dtype=torch.float32)\n",
    "y"
   ],
   "id": "a444f7074f9e8e27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:28:08.604681Z",
     "start_time": "2025-07-23T14:28:08.598774Z"
    }
   },
   "cell_type": "code",
   "source": "torch.dot(x,y)",
   "id": "3e5d3392c948c8ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:30:56.825482Z",
     "start_time": "2025-07-23T14:30:56.820871Z"
    }
   },
   "cell_type": "code",
   "source": "x*y",
   "id": "e93123b06fbef48b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:31:08.121257Z",
     "start_time": "2025-07-23T14:31:08.116418Z"
    }
   },
   "cell_type": "code",
   "source": "torch.sum(x*y)",
   "id": "f64b36f45dc35f84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 矩阵——向量积\n",
    "\n",
    "现在我们知道如何计算点积，可以开始理解矩阵-向量积 (matrix-vector product)。回顾分别在 (2.3.2)和 (2.3.1)中定义的矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 和向量 $\\mathbf{x} \\in \\mathbb{R}^n$。让我们将矩阵 $\\mathbf{A}$ 用它的行向量表示：\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} \\mathbf{a}_1^\\top \\\\ \\mathbf{a}_2^\\top \\\\ \\vdots \\\\ \\mathbf{a}_m^\\top \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "其中每个 $\\mathbf{a}_i^\\top \\in \\mathbb{R}^n$ 都是行向量，表示矩阵的第 $i$ 行。矩阵向量积 $\\mathbf{Ax}$ 是一个长度为 $m$ 的列向量，其第 $i$ 个元素是点积 $\\mathbf{a}_i^\\top \\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{Ax} = \\begin{bmatrix} \\mathbf{a}_1^\\top \\\\ \\mathbf{a}_2^\\top \\\\ \\vdots \\\\ \\mathbf{a}_m^\\top \\end{bmatrix} \\mathbf{x} = \\begin{bmatrix} \\mathbf{a}_1^\\top \\mathbf{x} \\\\ \\mathbf{a}_2^\\top \\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{a}_m^\\top \\mathbf{x} \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "### 举例说明\n",
    "\n",
    "这个公式说明，一个矩阵和一个向量相乘，其结果向量的每一个元素，都是由矩阵的对应行向量与那个向量做点积得到的。\n",
    "\n",
    "我们来看一个具体的例子。\n",
    "\n",
    "假设我们有矩阵 $\\mathbf{A}$ 和向量 $\\mathbf{x}$：\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "这里，矩阵 $\\mathbf{A}$ 是一个 $2 \\times 3$ 的矩阵，向量 $\\mathbf{x}$ 是一个 $3 \\times 1$ 的向量。\n",
    "\n",
    "根据公式，我们可以把矩阵 $\\mathbf{A}$ 看作是由两个行向量 $\\mathbf{a}_1^\\top$ 和 $\\mathbf{a}_2^\\top$ 堆叠而成的：\n",
    "\n",
    "*   第一个行向量是 $\\mathbf{a}_1^\\top = [1, 2, 3]$\n",
    "*   第二个行向量是 $\\mathbf{a}_2^\\top = [4, 5, 6]$\n",
    "\n",
    "现在，我们来计算矩阵-向量积 $\\mathbf{Ax}$。结果会是一个 $2 \\times 1$ 的向量，它的每个元素是 $\\mathbf{A}$ 的行向量和 $\\mathbf{x}$ 的点积：\n",
    "\n",
    "$$\n",
    "\\mathbf{Ax} = \\begin{bmatrix} \\mathbf{a}_1^\\top \\mathbf{x} \\\\ \\mathbf{a}_2^\\top \\mathbf{x} \\end{bmatrix} = \\begin{bmatrix} [1, 2, 3] \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} \\\\ [4, 5, 6] \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "我们分别计算这两个点积：\n",
    "\n",
    "1.  第一个元素：$\\mathbf{a}_1^\\top \\mathbf{x} = (1 \\times 7) + (2 \\times 8) + (3 \\times 9) = 7 + 16 + 27 = 50$\n",
    "2.  第二个元素：$\\mathbf{a}_2^\\top \\mathbf{x} = (4 \\times 7) + (5 \\times 8) + (6 \\times 9) = 28 + 40 + 54 = 122$\n",
    "\n",
    "所以，最终的结果是：\n",
    "\n",
    "$$\n",
    "\\mathbf{Ax} = \\begin{bmatrix} 50 \\\\ 122 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "这个例子清晰地展示了如何将矩阵的每一行与向量进行点积，从而得到最终的向量结果。"
   ],
   "id": "1114b5383c0c3624"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:41:25.662825Z",
     "start_time": "2025-07-23T14:41:25.657979Z"
    }
   },
   "cell_type": "code",
   "source": "A",
   "id": "f9cc3c2a56ff98c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:41:34.076336Z",
     "start_time": "2025-07-23T14:41:34.072219Z"
    }
   },
   "cell_type": "code",
   "source": "A.shape,x.shape",
   "id": "e40c459fd72cba22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:41:46.182539Z",
     "start_time": "2025-07-23T14:41:46.177172Z"
    }
   },
   "cell_type": "code",
   "source": "torch.mv(A,x)",
   "id": "1f106fe1cb214df1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 14.,  38.,  62.,  86., 110.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 矩阵乘法\n",
    "\n",
    "在掌握点积和矩阵-向量积的知识后，那么矩阵-矩阵乘法 (matrix-matrix multiplication) 应该很简单。\n",
    "\n",
    "假设有两个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ 和 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nk}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{k1} & b_{k2} & \\cdots & b_{km}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "用行向量 $\\mathbf{a}_i^\\top \\in \\mathbb{R}^k$ 表示矩阵 $\\mathbf{A}$ 的第 $i$ 行，并让列向量 $\\mathbf{b}_j \\in \\mathbb{R}^k$ 作为矩阵 $\\mathbf{B}$ 的第 $j$ 列。要生成矩阵积 $\\mathbf{C} = \\mathbf{AB}$，最简单的方法是考虑 $\\mathbf{A}$ 的行向量和 $\\mathbf{B}$ 的列向量：\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "\\mathbf{a}_1^\\top \\\\\n",
    "\\mathbf{a}_2^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}_n^\\top\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{B} = \\begin{bmatrix}\n",
    "\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "当我们简单地将每个元素 $c_{ij}$ 计算为点积 $\\mathbf{a}_i^\\top \\mathbf{b}_j$:\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}_1^\\top \\\\\n",
    "\\mathbf{a}_2^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}_n^\\top\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{b}_1 & \\mathbf{b}_2 & \\cdots & \\mathbf{b}_m\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf{a}_1^\\top \\mathbf{b}_1 & \\mathbf{a}_1^\\top \\mathbf{b}_2 & \\cdots & \\mathbf{a}_1^\\top \\mathbf{b}_m \\\\\n",
    "\\mathbf{a}_2^\\top \\mathbf{b}_1 & \\mathbf{a}_2^\\top \\mathbf{b}_2 & \\cdots & \\mathbf{a}_2^\\top \\mathbf{b}_m \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{a}_n^\\top \\mathbf{b}_1 & \\mathbf{a}_n^\\top \\mathbf{b}_2 & \\cdots & \\mathbf{a}_n^\\top \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "我们可以将矩阵-矩阵乘法 $\\mathbf{AB}$ 看作简单地执行 $m$ 次矩阵-向量积，并将结果拼接在一起，形成一个 $n \\times m$ 矩阵。在下面的代码中，我们在A和B上执行矩阵乘法。这里的A是一个5行4列的矩阵，B是一个4行3列的矩阵。两者相乘后，我们得到了一个5行3列的矩阵。\n",
    "\n",
    "***\n",
    "\n",
    "### 举例说明\n",
    "\n",
    "这个公式的核心思想是：结果矩阵 $\\mathbf{C}$ 中第 $i$ 行、第 $j$ 列的元素 $c_{ij}$，是由第一个矩阵 $\\mathbf{A}$ 的第 $i$ 行与第二个矩阵 $\\mathbf{B}$ 的第 $j$ 列进行点积运算得到的。\n",
    "\n",
    "我们来看一个具体的例子。\n",
    "\n",
    "假设我们有两个矩阵 $\\mathbf{A}$ (2x3) 和 $\\mathbf{B}$ (3x2)：\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 7 & 8 \\\\ 9 & 10 \\\\ 11 & 12 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "要计算 $\\mathbf{C} = \\mathbf{AB}$，结果 $\\mathbf{C}$ 将会是一个 $2 \\times 2$ 的矩阵。\n",
    "\n",
    "首先，我们把 $\\mathbf{A}$ 分解为行向量：\n",
    "*   $\\mathbf{A}$ 的第1行: $\\mathbf{a}_1^\\top = [1, 2, 3]$\n",
    "*   $\\mathbf{A}$ 的第2行: $\\mathbf{a}_2^\\top = [4, 5, 6]$\n",
    "\n",
    "然后，把 $\\mathbf{B}$ 分解为列向量：\n",
    "*   $\\mathbf{B}$ 的第1列: $\\mathbf{b}_1 = \\begin{bmatrix} 7 \\\\ 9 \\\\ 11 \\end{bmatrix}$\n",
    "*   $\\mathbf{B}$ 的第2列: $\\mathbf{b}_2 = \\begin{bmatrix} 8 \\\\ 10 \\\\ 12 \\end{bmatrix}$\n",
    "\n",
    "现在，我们根据公式 $c_{ij} = \\mathbf{a}_i^\\top \\mathbf{b}_j$ 来计算 $\\mathbf{C}$ 的四个元素：\n",
    "\n",
    "1.  **$c_{11}$ (C的第一行第一列)** = $\\mathbf{A}$ 的第1行 $\\cdot$ $\\mathbf{B}$ 的第1列 = $\\mathbf{a}_1^\\top \\mathbf{b}_1$\n",
    "    $c_{11} = (1 \\times 7) + (2 \\times 9) + (3 \\times 11) = 7 + 18 + 33 = 58$\n",
    "\n",
    "2.  **$c_{12}$ (C的第一行第二列)** = $\\mathbf{A}$ 的第1行 $\\cdot$ $\\mathbf{B}$ 的第2列 = $\\mathbf{a}_1^\\top \\mathbf{b}_2$\n",
    "    $c_{12} = (1 \\times 8) + (2 \\times 10) + (3 \\times 12) = 8 + 20 + 36 = 64$\n",
    "\n",
    "3.  **$c_{21}$ (C的第二行第一列)** = $\\mathbf{A}$ 的第2行 $\\cdot$ $\\mathbf{B}$ 的第1列 = $\\mathbf{a}_2^\\top \\mathbf{b}_1$\n",
    "    $c_{21} = (4 \\times 7) + (5 \\times 9) + (6 \\times 11) = 28 + 45 + 66 = 139$\n",
    "\n",
    "4.  **$c_{22}$ (C的第二行第二列)** = $\\mathbf{A}$ 的第2行 $\\cdot$ $\\mathbf{B}$ 的第2列 = $\\mathbf{a}_2^\\top \\mathbf{b}_2$\n",
    "    $c_{22} = (4 \\times 8) + (5 \\times 10) + (6 \\times 12) = 32 + 50 + 72 = 154$\n",
    "\n",
    "最后，把这些结果组合成矩阵 $\\mathbf{C}$：\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\begin{bmatrix} 58 & 64 \\\\ 139 & 154 \\end{bmatrix}\n",
    "$$"
   ],
   "id": "b86b145345525b92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:51:24.776918Z",
     "start_time": "2025-07-23T14:51:24.772059Z"
    }
   },
   "cell_type": "code",
   "source": "A,A.shape",
   "id": "7b760eaf20f68b02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:51:29.261439Z",
     "start_time": "2025-07-23T14:51:29.256440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "B = torch.ones(4,3)\n",
    "B,B.shape"
   ],
   "id": "aac7885b0b30a98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T14:52:07.443669Z",
     "start_time": "2025-07-23T14:52:07.438419Z"
    }
   },
   "cell_type": "code",
   "source": "torch.mm(A,B)",
   "id": "6f054922e5f154b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 范数\n",
    "1. 绝对值缩放\n",
    "$$\n",
    "f\\left( \\alpha \\mathbf{x} \\right) =|\\alpha |\\mathbf{x}\n",
    "$$\n",
    "2. 三角不等式\n",
    "$$\n",
    "f\\left( \\mathbf{x}+\\mathbf{y} \\right) \\leqslant f\\left( \\mathbf{x} \\right) +f\\left( \\mathbf{y} \\right) \n",
    "$$\n",
    "3. 非负\n",
    "$$\n",
    "f\\left( \\mathbf{x} \\right) \\geqslant 0\n",
    "$$\n",
    "4. 最小为$0$\n",
    "$$\n",
    "\\forall i,\\left[ \\mathbf{x} \\right] _i=0\\leftrightarrow f\\left( \\mathbf{x} \\right) =0\n",
    "$$\n",
    "### L2范数\n",
    "$$\n",
    "\\left\\| \\mathbf{x} \\right\\| =\\left\\| \\mathbf{x} \\right\\| _2=\\sqrt{\\sum_{i=1}^n{x_{i}^{2}}}\n",
    "$$\n"
   ],
   "id": "40459f62b9cf28e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:03:18.961472Z",
     "start_time": "2025-07-23T15:03:18.955469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "u = torch.tensor([3.0,-4.0])\n",
    "torch.norm(u)"
   ],
   "id": "a88eb6060239388c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$L_1$范数:$$\n",
    "\\left\\| \\mathbf{x} \\right\\| _1=\\sum_{i=1}^n{|x_i|}\n",
    "$$\n"
   ],
   "id": "8b57ace6e306fe30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:04:34.377044Z",
     "start_time": "2025-07-23T15:04:34.371875Z"
    }
   },
   "cell_type": "code",
   "source": "torch.abs(u).sum()",
   "id": "ff4eba0e92287332",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$L_P$ 范数\n",
    "$$\n",
    "\\left\\| \\mathbf{x} \\right\\| _p=\\left( \\sum_{i=1}^n{|x_i|^p} \\right) ^{\\frac{1}{p}}\n",
    "\\\\\n",
    "$$\n",
    "$Forbenius范数$ 是矩阵元素平方和的平方根\n",
    "\n",
    "$$\n",
    "\\left\\| \\mathbf{X} \\right\\| _F=\\sqrt{\\sum_{i=1}^m{\\sum_{j=1}^n{x_{ij}^{2}}}}\n",
    "$$\n"
   ],
   "id": "4500cfa1d617e4da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:08:35.067623Z",
     "start_time": "2025-07-23T15:08:35.063139Z"
    }
   },
   "cell_type": "code",
   "source": "torch.norm(torch.ones(4,9))",
   "id": "8e983614bbf81203",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 练习\n",
    "\n",
    "### 1. 证明：一个矩阵A的转置的转置是A\n",
    "$$\n",
    "(\\mathbf{A}^\\top)^\\top = \\mathbf{A}\n",
    "$$\n",
    "\n",
    "**直观解释：**\n",
    "矩阵的转置操作就是将其行和列进行互换。可以想象成将整个矩阵沿主对角线（从左上到右下）翻转一次。如果连续翻转两次，它自然就回到了原来的位置。\n",
    "\n",
    "**形式化证明：**\n",
    "我们通过矩阵的元素来证明。假设矩阵 $\\mathbf{A}$ 是一个 $m \\times n$ 的矩阵，其在第 $i$ 行第 $j$ 列的元素记为 $(\\mathbf{A})_{ij}$。\n",
    "\n",
    "1.  根据转置的定义，$\\mathbf{A}^\\top$ 是一个 $n \\times m$ 的矩阵，其在第 $i$ 行第 $j$ 列的元素为：\n",
    "    $$\n",
    "    (\\mathbf{A}^\\top)_{ij} = (\\mathbf{A})_{ji}\n",
    "    $$\n",
    "2.  现在对 $\\mathbf{A}^\\top$ 再次进行转置，得到 $(\\mathbf{A}^\\top)^\\top$。这是一个 $m \\times n$ 的矩阵。其在第 $i$ 行第 $j$ 列的元素为：\n",
    "    $$\n",
    "    ((\\mathbf{A}^\\top)^\\top)_{ij} = (\\mathbf{A}^\\top)_{ji}\n",
    "    $$\n",
    "3.  结合第一步的结论，我们将 $(\\mathbf{A}^\\top)_{ji}$ 替换为原始矩阵 $\\mathbf{A}$ 的元素，即 $(\\mathbf{A})_{ij}$。\n",
    "    $$\n",
    "    ((\\mathbf{A}^\\top)^\\top)_{ij} = (\\mathbf{A})_{ij}\n",
    "    $$\n",
    "由于这个等式对所有的 $i$ 和 $j$ 都成立，因此证明了 $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$。\n",
    "\n",
    "\n",
    "\n",
    "### 2. 证明：“它们转置的和”等于“它们和的转置”\n",
    "$$\n",
    "\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top\n",
    "$$\n",
    "\n",
    "**直观解释：**\n",
    "矩阵加法是逐个元素相加的。你可以先将两个矩阵对应位置的元素相加，然后将结果矩阵进行行列互换；也可以先将两个矩阵分别进行行列互换，然后再将它们相加。由于加法和行列互换这两个操作的顺序不影响最终每个元素的值，所以结果是相同的。\n",
    "\n",
    "**形式化证明:**\n",
    "假设 $\\mathbf{A}$ 和 $\\mathbf{B}$ 是两个同型的 $m \\times n$ 矩阵。\n",
    "\n",
    "1.  我们先看等式右边 $(\\mathbf{A} + \\mathbf{B})^\\top$。令 $\\mathbf{C} = \\mathbf{A} + \\mathbf{B}$，则 $\\mathbf{C}$ 的每个元素为 $(\\mathbf{C})_{ij} = (\\mathbf{A})_{ij} + (\\mathbf{B})_{ij}$。\n",
    "2.  对 $\\mathbf{C}$ 进行转置，其在第 $i$ 行第 $j$ 列的元素为：\n",
    "    $$\n",
    "    (\\mathbf{C}^\\top)_{ij} = (\\mathbf{C})_{ji} = (\\mathbf{A})_{ji} + (\\mathbf{B})_{ji}\n",
    "    $$\n",
    "3.  现在看等式左边 $\\mathbf{A}^\\top + \\mathbf{B}^\\top$。根据转置的定义，我们知道 $(\\mathbf{A}^\\top)_{ij} = (\\mathbf{A})_{ji}$ 且 $(\\mathbf{B}^\\top)_{ij} = (\\mathbf{B})_{ji}$。\n",
    "4.  将这两个转置矩阵相加，得到的新矩阵在第 $i$ 行第 $j$ 列的元素为：\n",
    "    $$\n",
    "    (\\mathbf{A}^\\top + \\mathbf{B}^\\top)_{ij} = (\\mathbf{A}^\\top)_{ij} + (\\mathbf{B}^\\top)_{ij} = (\\mathbf{A})_{ji} + (\\mathbf{B})_{ji}\n",
    "    $$\n",
    "\n",
    "比较第二步和第四步的结果，可以看出两边矩阵在任意位置 $(i, j)$ 的元素都相等，因此这两个矩阵是相等的。\n",
    "\n",
    "\n",
    "### 3. 给定任意方阵A, A + Aᵀ 总是对称的吗？为什么？\n",
    "\n",
    "**是的，对于任意方阵 $\\mathbf{A}$，矩阵 $\\mathbf{A} + \\mathbf{A}^\\top$ 总是对称的。**\n",
    "\n",
    "**解释与证明：**\n",
    "首先，我们回顾一下**对称矩阵**的定义：一个方阵 $\\mathbf{S}$ 如果满足其转置等于自身，即 $\\mathbf{S}^\\top = \\mathbf{S}$，那么它就是对称矩阵。\n",
    "\n",
    "现在，我们需要证明 $\\mathbf{A} + \\mathbf{A}^\\top$ 满足这个定义。令一个新的矩阵 $\\mathbf{C} = \\mathbf{A} + \\mathbf{A}^\\top$。我们的目标是证明 $\\mathbf{C}^\\top = \\mathbf{C}$。\n",
    "\n",
    "1.  对 $\\mathbf{C}$ 进行转置：\n",
    "    $$\n",
    "    \\mathbf{C}^\\top = (\\mathbf{A} + \\mathbf{A}^\\top)^\\top\n",
    "    $$\n",
    "2.  根据上一问证明的性质（和的转置等于转置的和），我们可以将上式展开：\n",
    "    $$\n",
    "    (\\mathbf{A} + \\mathbf{A}^\\top)^\\top = \\mathbf{A}^\\top + (\\mathbf{A}^\\top)^\\top\n",
    "    $$\n",
    "3.  再根据第一问证明的性质（转置的转置等于原矩阵），我们得到：\n",
    "    $$\n",
    "    \\mathbf{A}^\\top + (\\mathbf{A}^\\top)^\\top = \\mathbf{A}^\\top + \\mathbf{A}\n",
    "    $$\n",
    "4.  由于矩阵加法满足交换律，$\\mathbf{A}^\\top + \\mathbf{A} = \\mathbf{A} + \\mathbf{A}^\\top$。\n",
    "5.  我们发现，这个结果正是我们定义的原始矩阵 $\\mathbf{C}$。\n",
    "\n",
    "所以，我们完成了证明：\n",
    "$$\n",
    "\\mathbf{C}^\\top = \\mathbf{A} + \\mathbf{A}^\\top = \\mathbf{C}\n",
    "$$\n",
    "因为 $\\mathbf{C}^\\top = \\mathbf{C}$，所以 $\\mathbf{A} + \\mathbf{A}^\\top$ 总是一个对称矩阵。\n",
    "\n",
    "\n",
    "### 4. 本节中定义了形状(2, 3, 4)的张量X。len(X)的输出结果是什么？\n",
    "\n",
    "`len(X)` 的输出结果是 **2**。\n",
    "\n",
    "**解释:**\n",
    "在 Python 中，`len()` 函数通常返回一个对象的第一维（或最外层）的长度。对于 PyTorch 张量（或 NumPy 数组），`len()` 会返回其第 0 轴 (axis 0) 的大小。\n"
   ],
   "id": "86fe9489b76f41ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:16:49.597365Z",
     "start_time": "2025-07-23T15:16:49.592361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个形状为 (2, 3, 4) 的张量\n",
    "X = torch.randn(2, 3, 4)\n",
    "\n",
    "print(\"张量的形状:\", X.shape)\n",
    "print(\"len(X) 的输出:\", len(X))"
   ],
   "id": "69147d79d632a88f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量的形状: torch.Size([2, 3, 4])\n",
      "len(X) 的输出: 2\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 5. 对于任意形状的张量X, len(X)是否总是对应于X特定轴的长度？这个轴是什么？\n",
    "\n",
    "是的，`len(X)` **总是对应于张量 X 特定轴的长度**。\n",
    "\n",
    "这个轴是 **第0轴 (axis=0)**。\n",
    "\n",
    "无论是二维矩阵、三维张量还是更高维度的张量，`len()` 函数始终返回第一个维度的长度。\n",
    "\n",
    "### 6. 运行 `A/A.sum(axis=1)`，看看会发生什么。请分析一下原因？\n",
    "其中 `A = torch.arange(20).reshape(5, 4)`\n",
    "\n",
    "运行这段代码会**引发一个运行时错误 (RuntimeError)**。\n",
    "\n",
    "**原因分析:**\n",
    "这涉及到 PyTorch/NumPy 中的**广播机制 (Broadcasting)**。当对两个形状不同的张量进行运算时，系统会尝试“广播”较小的张量，通过复制其数据使其形状与较大的张量兼容。\n",
    "\n",
    "1.  **分析 A 的形状**:\n",
    "    `A` 是一个 `5x4` 的矩阵。\n",
    "    ```\n",
    "    A = tensor([[ 0,  1,  2,  3],\n",
    "                [ 4,  5,  6,  7],\n",
    "                [ 8,  9, 10, 11],\n",
    "                [12, 13, 14, 15],\n",
    "                [16, 17, 18, 19]])\n",
    "    A.shape -> torch.Size([5, 4])\n",
    "    ```\n",
    "\n",
    "2.  **分析 `A.sum(axis=1)` 的形状**:\n",
    "    `axis=1` 表示沿着**行**对元素求和（即对每一行的所有列求和）。\n",
    "    ```\n",
    "    A.sum(axis=1) = tensor([ 6, 22, 38, 54, 70])\n",
    "    A.sum(axis=1).shape -> torch.Size([5])\n",
    "    ```\n",
    "    我们得到了一个长度为5的一维张量。\n",
    "\n",
    "3.  **广播机制如何工作**:\n",
    "    当计算 `A / A.sum(axis=1)` 时，PyTorch 尝试匹配它们的形状。它从**尾部维度 (trailing dimensions)** 开始比较：\n",
    "    *   `A` 的形状: `(5, 4)`\n",
    "    *   `A.sum(axis=1)` 的形状: `(5)`\n",
    "\n",
    "    PyTorch 的广播规则无法将 `(5, 4)` 和 `(5)` 对齐进行逐元素除法。尾部维度 `4` 和 `5` 不匹配，并且其中一个不为 `1`，因此无法广播。\n",
    "\n",
    "**如何修正这个问题？**\n",
    "如果我们想让每一行的元素都除以该行的总和，我们需要保持求和后的维度，使其形状变为 `(5, 1)`。这可以通过设置 `keepdim=True` 来实现。\n",
    "这样，一个 `(5, 1)` 的张量就可以和 `(5, 4)` 的张量进行运算了，因为 `1` 可以被广播到 `4`。\n",
    "\n"
   ],
   "id": "b5fec9ccbed3c1d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:18:14.571742Z",
     "start_time": "2025-07-23T15:18:14.564741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "\n",
    "# 这样会报错\n",
    "try:\n",
    "    C = A / A.sum(axis=1)\n",
    "except RuntimeError as e:\n",
    "    print(\"发生了错误:\", e)\n",
    "\n",
    "# 正确的做法\n",
    "A_sum = A.sum(axis=1, keepdim=True)\n",
    "print(\"\\nA.sum(axis=1, keepdim=True) 的形状:\", A_sum.shape)\n",
    "# (5, 4) / (5, 1) -> (5, 4) \n",
    "# (5, 1) 的列被广播(复制)4次来匹配 (5, 4) 的形状\n",
    "C_correct = A / A_sum\n",
    "print(\"\\n正确的计算结果:\\n\", C_correct)"
   ],
   "id": "84326ffae8cc1d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发生了错误: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1\n",
      "\n",
      "A.sum(axis=1, keepdim=True) 的形状: torch.Size([5, 1])\n",
      "\n",
      "正确的计算结果:\n",
      " tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
      "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
      "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
      "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
      "        [0.2286, 0.2429, 0.2571, 0.2714]])\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. 考虑一个具有形状(2, 3, 4)的张量，在轴0、1、2上的求和输出是什么形状？\n",
    "\n",
    "假设张量为 `X`，形状为 `(2, 3, 4)`。在某个轴上求和，会“压缩”掉该轴，使其维度消失。\n",
    "\n",
    "*   **在轴 `0` 上求和**: `X.sum(axis=0)`\n",
    "    形状从 `(2, 3, 4)` 变为 `(3, 4)`。\n",
    "\n",
    "*   **在轴 `1` 上求和**: `X.sum(axis=1)`\n",
    "    形状从 `(2, 3, 4)` 变为 `(2, 4)`。\n",
    "\n",
    "*   **在轴 `2` 上求和**: `X.sum(axis=2)`\n",
    "    形状从 `(2, 3, 4)` 变为 `(2, 3)`。\n",
    "\n",
    "---\n",
    "### 8. 为 `linalg.norm` 函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么？\n",
    "\n",
    "`torch.linalg.norm` (或其别名 `torch.norm`) 在不指定任何轴的情况下，计算的是张量中**所有元素的 L2 范数**，也称为 **Frobenius 范数** 的推广。\n",
    "\n",
    "对于一个任意张量 $\\mathbf{X}$，其计算公式为：\n",
    "$$\n",
    "||\\mathbf{X}|| = \\sqrt{\\sum_{i_1, i_2, \\dots, i_n} |x_{i_1, i_2, \\dots, i_n}|^2}\n",
    "$$\n",
    "这本质上是将张量“展平”成一个长向量，然后计算这个向量的欧几里得距离（L2范数）。\n",
    "\n",
    "**观察输出:**\n",
    "无论输入张量的维度有多少（是3维、4维还是更高维），输出都是一个**标量**（0维张量），代表整个张量的范数。\n",
    "\n",
    "\n",
    "**总结:** 对于任意形状的张量，`linalg.norm` 函数默认计算的是一个单一的数值，该数值是对张量中所有元素平方求和再开方的结果，衡量了张量整体的大小。"
   ],
   "id": "355edb28cea11330"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:19:29.772742Z",
     "start_time": "2025-07-23T15:19:29.764829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 3 轴张量 (2, 3, 4)\n",
    "X_3d = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)\n",
    "\n",
    "# 计算它的范数\n",
    "norm_3d = torch.linalg.norm(X_3d)\n",
    "\n",
    "print(\"3D 张量:\\n\", X_3d)\n",
    "print(\"\\n它的范数:\", norm_3d)\n",
    "print(\"范数的形状:\", norm_3d.shape) # shape 是 torch.Size([]) 表示标量\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 我们可以手动验证\n",
    "# 先计算所有元素的平方和，然后开方\n",
    "manual_norm = torch.sqrt(torch.sum(X_3d ** 2))\n",
    "print(\"手动计算的范数:\", manual_norm)\n",
    "print(\"两次计算结果是否接近:\", torch.allclose(norm_3d, manual_norm))"
   ],
   "id": "f686bcbbd128b49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D 张量:\n",
      " tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.]]])\n",
      "\n",
      "它的范数: tensor(65.7571)\n",
      "范数的形状: torch.Size([])\n",
      "--------------------\n",
      "手动计算的范数: tensor(65.7571)\n",
      "两次计算结果是否接近: True\n"
     ]
    }
   ],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
