{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 自动微分\n",
    "深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。实际中，根据设计好的模型，系统会构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。"
   ],
   "id": "4ed846bc0432106d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对函数$y=2\\mathbf{x}^{\\mathbf{T}}\\mathbf{x}$ 对列向量 $\\mathbf{x}$ 求导",
   "id": "199e4aeec9cffc43"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:21.003659Z",
     "start_time": "2025-07-24T15:08:18.982652Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:22.354984Z",
     "start_time": "2025-07-24T15:08:22.348983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 存储梯度\n",
    "x.requires_grad_(True)"
   ],
   "id": "48ec6497ccea3a8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:22.635435Z",
     "start_time": "2025-07-24T15:08:22.631483Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad  # 默认None",
   "id": "4e10f0ed95cba008",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:23.390261Z",
     "start_time": "2025-07-24T15:08:23.384260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = 2 * torch.dot(x, x)  # 2*torch.dot(x.T,x)也是可以的,一维张量转置不改变任何东西\n",
    "y"
   ],
   "id": "ac06752e2e1918a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:23.827920Z",
     "start_time": "2025-07-24T15:08:23.786462Z"
    }
   },
   "cell_type": "code",
   "source": "y.backward()",
   "id": "61037a6e6d2fc58",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:24.026074Z",
     "start_time": "2025-07-24T15:08:24.021052Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "a0747602a815ecf9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:24.215088Z",
     "start_time": "2025-07-24T15:08:24.209083Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad.zero_()",
   "id": "6eccdab75ca770c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:24.391367Z",
     "start_time": "2025-07-24T15:08:24.388008Z"
    }
   },
   "cell_type": "code",
   "source": "y = x.sum()",
   "id": "c6edc9c3b881dcaf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:24.559071Z",
     "start_time": "2025-07-24T15:08:24.554088Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "8d3be5740c9b1df3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:24.763621Z",
     "start_time": "2025-07-24T15:08:24.759621Z"
    }
   },
   "cell_type": "code",
   "source": "y.backward()",
   "id": "4035827c5c7901ea",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:25.048289Z",
     "start_time": "2025-07-24T15:08:25.042396Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "30a4b80f0f00a51d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 非标量变量的反向传播",
   "id": "d68cd4901a05dc26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:25.510670Z",
     "start_time": "2025-07-24T15:08:25.505673Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad.zero_()",
   "id": "cca46765894ad779",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:25.689385Z",
     "start_time": "2025-07-24T15:08:25.685383Z"
    }
   },
   "cell_type": "code",
   "source": "y = x * x",
   "id": "e0f10e3586c9795a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:26.376124Z",
     "start_time": "2025-07-24T15:08:26.371031Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "38fc230648d5f74f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:26.962951Z",
     "start_time": "2025-07-24T15:08:26.719404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 试一下向量对向量求导\n",
    "y.backward()  #报错的"
   ],
   "id": "3147a294fbede6c0",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 试一下向量对向量求导\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m#报错的\u001B[39;00m\n",
      "File \u001B[1;32mG:\\dl\\deeplearning\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\dl\\deeplearning\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:346\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    337\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    338\u001B[0m     (inputs,)\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, (torch\u001B[38;5;241m.\u001B[39mTensor, graph\u001B[38;5;241m.\u001B[39mGradientEdge))\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m ()\n\u001B[0;32m    343\u001B[0m )\n\u001B[0;32m    345\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m--> 346\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[1;32mG:\\dl\\deeplearning\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:199\u001B[0m, in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m    197\u001B[0m     out_numel_is_1 \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m out_numel_is_1:\n\u001B[1;32m--> 199\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    200\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    201\u001B[0m     )\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m out_dtype\u001B[38;5;241m.\u001B[39mis_floating_point:\n\u001B[0;32m    203\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    204\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for real scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    205\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout_dtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    206\u001B[0m     )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:29.220046Z",
     "start_time": "2025-07-24T15:08:29.216004Z"
    }
   },
   "cell_type": "code",
   "source": "y.sum().backward()",
   "id": "33ef931f7d5e0e1d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "我们定义的标量损失是 $L = \\sum_{i=0}^{3} y_i = \\sum_{i=0}^{3} x_i^2$。\n",
    "现在计算 $L$ 对每个 $x_j$ 的梯度：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (x_0^2 + x_1^2 + x_2^2 + x_3^2) = 2x_j\n",
    "$$\n",
    "所以，$\\mathbf{x}$ 的梯度向量应该是 $[2x_0, 2x_1, 2x_2, 2x_3] =$，这与代码输出 `tensor([0., 2., 4., 6.])` 完全一致。"
   ],
   "id": "31330d2b96c95f80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:32.814179Z",
     "start_time": "2025-07-24T15:08:32.808181Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "a10deee33e4ae224",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 分离计算图\n",
    "有时，我们希望将某些计算移动到记录的计算图之外。例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用。"
   ],
   "id": "57eb41ae6ec420f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:34.168396Z",
     "start_time": "2025-07-24T15:08:34.159176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 1. 创建一个需要梯度的张量 a\n",
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(f\"原始张量 a: {a}\")\n",
    "\n",
    "# 2. 对 a 进行操作得到 b\n",
    "b = a * 2\n",
    "print(f\"张量 b: {b}\")  # b 继承了 a 的梯度属性，是计算图的一部分\n",
    "\n",
    "# 3. 使用 detach() 创建 c\n",
    "c = b.detach()\n",
    "print(f\"分离后的张量 c: {c}\")\n",
    "print(f\"a.requires_grad: {a.requires_grad}\")  # True\n",
    "print(f\"b.requires_grad: {b.requires_grad}\")  # True\n",
    "print(f\"c.requires_grad: {c.requires_grad}\")  # False\n",
    "\n",
    "# 4. 证明 c 和 b 共享数据内存\n",
    "# 修改 c 的数据，b 也会改变\n",
    "c[0] = 100.0\n",
    "print(f\"修改 c 后, b 的值: {b}\")  # b 的值也变了\n",
    "print(f\"修改 c 后, c 的值: {c}\")\n",
    "\n",
    "# 5. 梯度计算的对比\n",
    "d = b.sum()\n",
    "d.backward()  # 从 d 反向传播\n",
    "print(f\"a 的梯度 a.grad: {a.grad}\")  # tensor([2., 2., 2.])\n",
    "\n",
    "# 如果我们尝试对 c 进行反向传播，会发生什么？\n",
    "try:\n",
    "    e = c.sum()\n",
    "    e.backward()\n",
    "except RuntimeError as e:\n",
    "    # 这会报错，因为 c 不在计算图中，且 requires_grad=False\n",
    "    print(f\"\\n对 c 求和并反向传播时出错: {e}\")"
   ],
   "id": "817ebfeb41694699",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量 a: tensor([1., 2., 3.], requires_grad=True)\n",
      "张量 b: tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n",
      "分离后的张量 c: tensor([2., 4., 6.])\n",
      "a.requires_grad: True\n",
      "b.requires_grad: True\n",
      "c.requires_grad: False\n",
      "修改 c 后, b 的值: tensor([100.,   4.,   6.], grad_fn=<MulBackward0>)\n",
      "修改 c 后, c 的值: tensor([100.,   4.,   6.])\n",
      "a 的梯度 a.grad: tensor([2., 2., 2.])\n",
      "\n",
      "对 c 求和并反向传播时出错: element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:34.584393Z",
     "start_time": "2025-07-24T15:08:34.578394Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad.zero_()",
   "id": "32392f627e441aaf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:34.780569Z",
     "start_time": "2025-07-24T15:08:34.776502Z"
    }
   },
   "cell_type": "code",
   "source": "y = x * x",
   "id": "3b31101b36f625f4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:35.007376Z",
     "start_time": "2025-07-24T15:08:35.001374Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "120a7fbc3b5da082",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:35.193992Z",
     "start_time": "2025-07-24T15:08:35.189592Z"
    }
   },
   "cell_type": "code",
   "source": "u = y.detach()",
   "id": "7369c8cc3c9ed552",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:35.357427Z",
     "start_time": "2025-07-24T15:08:35.352152Z"
    }
   },
   "cell_type": "code",
   "source": "u",
   "id": "d75d565b7236e61c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:35.549323Z",
     "start_time": "2025-07-24T15:08:35.545320Z"
    }
   },
   "cell_type": "code",
   "source": "z = u * x  # u 被看作是常数了",
   "id": "30a09ebd358d002",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:35.750979Z",
     "start_time": "2025-07-24T15:08:35.745869Z"
    }
   },
   "cell_type": "code",
   "source": "z",
   "id": "e19354e9c4b6ed1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:35.921539Z",
     "start_time": "2025-07-24T15:08:35.916540Z"
    }
   },
   "cell_type": "code",
   "source": "z.sum().backward()",
   "id": "2845e24fd9cef1be",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:36.093850Z",
     "start_time": "2025-07-24T15:08:36.087854Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "9da83e396f3d1334",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Python 控制流的梯度计算",
   "id": "6bd4b398baf8f923"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:36.380618Z",
     "start_time": "2025-07-24T15:08:36.376787Z"
    }
   },
   "cell_type": "code",
   "source": "a = torch.randn(size=(), requires_grad=True)",
   "id": "2bd0b203e7d4ad70",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:36.639505Z",
     "start_time": "2025-07-24T15:08:36.634472Z"
    }
   },
   "cell_type": "code",
   "source": "a",
   "id": "adaa867004132031",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6075, requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:36.819940Z",
     "start_time": "2025-07-24T15:08:36.815119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def f(a: Tensor):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ],
   "id": "57421e162ca99da6",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:36.961074Z",
     "start_time": "2025-07-24T15:08:36.956890Z"
    }
   },
   "cell_type": "code",
   "source": "d = f(a)",
   "id": "3efde377d16c63e5",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:37.095992Z",
     "start_time": "2025-07-24T15:08:37.090561Z"
    }
   },
   "cell_type": "code",
   "source": "d",
   "id": "9efed28801cc95aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1646.0342, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:37.249019Z",
     "start_time": "2025-07-24T15:08:37.245021Z"
    }
   },
   "cell_type": "code",
   "source": "d.backward()",
   "id": "4a7a9520494305e2",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:37.347827Z",
     "start_time": "2025-07-24T15:08:37.341826Z"
    }
   },
   "cell_type": "code",
   "source": "a.grad",
   "id": "ac526d166fa4ff15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1024.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:37.517588Z",
     "start_time": "2025-07-24T15:08:37.511592Z"
    }
   },
   "cell_type": "code",
   "source": "d/a",
   "id": "f87082e0ce719fd3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1024., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 课后练习\n",
    " 1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "\n",
    "计算二阶导数的开销之所以更大，主要是因为它涉及了**两次反向传播过程**，并且需要在第一次反向传播时**构建一个额外的计算图**。\n",
    "\n",
    "让我们从计算图的角度来理解这个过程：\n",
    "\n",
    "1.  **计算一阶导数**:\n",
    "    *   **前向传播**: 我们从输入 $x$ 开始，通过一系列运算得到输出 $y = f(x)$。PyTorch会构建一个计算图，记录下从 $x$ 到 $y$ 的所有操作。\n",
    "    *   **反向传播**: 当我们调用 `y.backward()` 时，Autograd引擎会沿着这个图反向遍历，计算出 $y$ 对 $x$ 的梯度 $\\frac{dy}{dx}$。计算完成后，为了节约内存，这个图默认会被**释放**。\n",
    "    *   这个过程可以看作是：$x \\xrightarrow{\\text{构建图}} f(x) \\xrightarrow{\\text{反向遍历一次}} \\frac{df(x)}{dx}$。\n",
    "\n",
    "2.  **计算二阶导数**:\n",
    "    *   二阶导数是**一阶导数的导数**，即 $\\frac{d^2y}{dx^2} = \\frac{d}{dx}(\\frac{dy}{dx})$。\n",
    "    *   为了计算它，我们首先需要得到一阶导数 $\\frac{dy}{dx}$。但关键是，我们不能只得到它的数值，我们还需要得到**如何从 $x$ 计算出 $\\frac{dy}{dx}$ 的计算图**。\n",
    "    *   **第一次反向传播 (`create_graph=True`)**: 我们在第一次调用 `backward` 时，必须设置参数 `create_graph=True`。这会告诉PyTorch：在计算一阶导数的同时，请为这个求导过程本身也构建一个计算图。这个新图描述了一阶导数 $\\frac{dy}{dx}$ 是如何作为 $x$ 的函数计算出来的。\n",
    "    *   **第二次反向传播**: 现在我们有了一阶导数（我们称之为 `grad_x`）和它自己的计算图。我们再对 `grad_x` 进行反向传播，即调用 `grad_x.backward()`，就能得到二阶导数 $\\frac{d(grad\\_x)}{dx}$。\n",
    "    *   这个过程可以看作是：\n",
    "        $$\n",
    "        x \\xrightarrow{\\text{(1) 正向}} f(x) \\xrightarrow[\\text{同时构建梯度图}]{\\text{(2) 第一次反向}} \\frac{df(x)}{dx} \\xrightarrow{\\text{(3) 第二次反向}} \\frac{d^2f(x)}{dx^2}\n",
    "        $$\n",
    "\n",
    "**开销大的原因总结**:\n",
    "*   **双倍的反向传播**: 执行了两次完整的反向计算过程。\n",
    "*   **创建额外的图**: 第一次反向传播的计算量更大，因为它不仅要计算梯度值，还要构建一个全新的、描述梯度计算过程的计算图。这个图的创建和存储会消耗更多的计算资源和内存。\n"
   ],
   "id": "b9ca336bfff078da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " 2. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "\n",
    "默认情况下，在 `.backward()` 执行完毕后，PyTorch为了提高效率、节省内存，会自动**销毁**用于计算梯度的计算图。如果尝试立即再次运行它，会触发一个 `RuntimeError`。\n",
    "\n",
    "**会发生什么**:\n",
    "程序会报错，错误信息通常是：\n",
    "`RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.`\n",
    "\n",
    "**解释**:\n",
    "这个错误信息非常明确：你试图对一个图进行第二次反向传播，但是存储在图中的中间结果（比如前向传播时各节点的输出值，它们是计算梯度所必需的）已经被释放了。\n",
    "\n",
    "**如何解决**:\n",
    "如果你确实需要对同一个图进行多次反向传播（例如，在一些高级应用中，一个网络的多个输出需要分别计算梯度），你需要在**第一次**调用 `backward()` 时指定 `retain_graph=True`。"
   ],
   "id": "3a0d2b25cb4907f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:38.642633Z",
     "start_time": "2025-07-24T15:08:38.635804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 3\n",
    "z = y * 2\n",
    "\n",
    "# 第一次反向传播\n",
    "z.backward(retain_graph=True) # retain_graph=True 保留了计算图\n",
    "print(f\"第一次梯度 (dz/dx): {x.grad.item()}\")\n",
    "\n",
    "# 重新清零梯度，否则梯度会累加\n",
    "x.grad.zero_() \n",
    "\n",
    "# 立即再次运行它（因为图被保留了，所以这次会成功）\n",
    "z.backward() \n",
    "print(f\"第二次梯度 (dz/dx): {x.grad.item()}\")"
   ],
   "id": "50a2821f91013ebb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一次梯度 (dz/dx): 24.0\n",
      "第二次梯度 (dz/dx): 24.0\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. 在控制流的例子中，如果将变量a更改为随机向量或矩阵，会发生什么？\n",
    "\n",
    "在上一节的控制流例子中，输入 `a` 和输出 `d` 都是标量（scalar）。如果将 `a` 改为向量或矩阵，会发生两件事：\n",
    "\n",
    "1.  **控制流本身依然可以正常工作**：函数 `f(a)` 中的运算对向量和矩阵是有定义的。\n",
    "    *   `b = a * 2`: 逐元素乘法，`b` 也是一个向量/矩阵。\n",
    "    *   `b.norm()`: 计算 `b` 的L2范数，结果是一个**标量**。`while` 循环的条件判断依然有效。\n",
    "    *   `b.sum()`: 计算 `b` 所有元素的和，结果也是一个**标量**。`if` 条件判断依然有效。\n",
    "    *   因此，前向传播 `d = f(a)` 会成功执行，输出的 `d` 将是一个和 `a` 形状相同的向量或矩阵。\n",
    "\n",
    "2.  **反向传播 `d.backward()` 会失败**：这是最关键的变化。`backward()` 方法默认只能被**标量**调用。当 `d` 是一个向量或矩阵时，直接调用 `d.backward()` 会触发 `RuntimeError`。\n",
    "\n",
    "**会发生什么**:\n",
    "程序会报错，错误信息通常是：\n",
    "`RuntimeError: grad can be implicitly created only for scalar outputs`\n",
    "\n",
    "**解释**:\n",
    "梯度的概念在数学上是指一个**标量函数**关于其自变量（可以是向量或矩阵）的变化率。当函数输出是向量（例如 $d = [d_1, d_2, ..., d_n]$）时，我们无法直接定义一个“总梯度”。我们能定义的是一个**雅可比矩阵 (Jacobian Matrix)**，其中每个元素 $J_{ij} = \\frac{\\partial d_i}{\\partial a_j}$。\n",
    "\n",
    "PyTorch的 `.backward()` 是为深度学习中优化**标量损失函数**而设计的。它实际上计算的是梯度与一个向量的乘积，即**雅可比-向量积 (Jacobian-vector product)**。当输出是标量时，这个向量默认为1。\n",
    "\n",
    "**如何解决**:\n",
    "如果你想对一个向量输出求梯度，你有两种常见做法：\n",
    "*   **方法一（最常用）**: 先将向量输出聚合为一个标量，通常是求和 `d.sum()` 或者求均值 `d.mean()`，然后再对这个标量调用 `.backward()`。这在计算总损失时非常常见。\n",
    "*   **方法二**: 在调用 `backward()` 时提供一个与 `d` 形状相同的 `gradient` 张量（通常是全1的张量），如 `d.backward(gradient=torch.ones_like(d))`。这在效果上等同于 `d.sum().backward()`。"
   ],
   "id": "31687c9ee7b8a9b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:39.127514Z",
     "start_time": "2025-07-24T15:08:39.120573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "# a 是一个向量\n",
    "a = torch.randn(3, requires_grad=True)\n",
    "d = f(a) # 前向传播成功\n",
    "\n",
    "print(f\"Input a: {a}\")\n",
    "print(f\"Output d: {d}\")\n",
    "\n",
    "# d.backward() # <--- 这行会报错！\n",
    "\n",
    "# 正确的做法:\n",
    "d_sum = d.sum()\n",
    "d_sum.backward()\n",
    "print(f\"Gradient a.grad: {a.grad}\")"
   ],
   "id": "7fd96f0f4d9ea264",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a: tensor([ 0.6271, -0.1845,  1.2266], requires_grad=True)\n",
      "Output d: tensor([ 642.1505, -188.9527, 1256.0619], grad_fn=<MulBackward0>)\n",
      "Gradient a.grad: tensor([1024., 1024., 1024.])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "\n",
    "我们可以设计一个函数，它根据输入向量的元素总和来决定应用哪种非线性变换。\n",
    "\n",
    "**设计思路**:\n",
    "*   输入是一个向量 `x`。\n",
    "*   计算 `x` 的元素和 `x.sum()`。\n",
    "*   如果和为正，输出为 $y = x^2$。\n",
    "*   如果和为负，输出为 $y = x^3$。\n",
    "这是一个依赖于数据本身的控制流。"
   ],
   "id": "5aaf40a928be98b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:08:39.780958Z",
     "start_time": "2025-07-24T15:08:39.772453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def dynamic_polynomial(x):\n",
    "    \"\"\"\n",
    "    根据输入x的元素和，动态选择一个多项式函数。\n",
    "    \"\"\"\n",
    "    if x.sum() > 0:\n",
    "        print(\"执行路径: y = x^2\")\n",
    "        y = x.pow(2)\n",
    "    else:\n",
    "        print(\"执行路径: y = x^3\")\n",
    "        y = x.pow(3)\n",
    "    return y.sum() # 返回标量损失\n",
    "\n",
    "# 案例1: 元素和为正\n",
    "print(\"--- 案例 1: sum(x) > 0 ---\")\n",
    "x1 = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y1 = dynamic_polynomial(x1)\n",
    "y1.backward()\n",
    "\n",
    "print(f\"输入 x1: {x1.data}\")\n",
    "print(f\"输出 y1: {y1.item()}\")\n",
    "print(f\"梯度 x1.grad: {x1.grad}\") # 理论梯度: 2*x = [2.0, 4.0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# 案例2: 元素和为负\n",
    "print(\"--- 案例 2: sum(x) <= 0 ---\")\n",
    "x2 = torch.tensor([-1.0, -2.0], requires_grad=True)\n",
    "y2 = dynamic_polynomial(x2)\n",
    "y2.backward()\n",
    "\n",
    "print(f\"输入 x2: {x2.data}\")\n",
    "print(f\"输出 y2: {y2.item()}\")\n",
    "print(f\"梯度 x2.grad: {x2.grad}\") # 理论梯度: 3*x^2 = [3.0, 12.0]"
   ],
   "id": "78b4ae46e5797495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 案例 1: sum(x) > 0 ---\n",
      "执行路径: y = x^2\n",
      "输入 x1: tensor([1., 2.])\n",
      "输出 y1: 5.0\n",
      "梯度 x1.grad: tensor([2., 4.])\n",
      "\n",
      "==============================\n",
      "\n",
      "--- 案例 2: sum(x) <= 0 ---\n",
      "执行路径: y = x^3\n",
      "输入 x2: tensor([-1., -2.])\n",
      "输出 y2: -9.0\n",
      "梯度 x2.grad: tensor([ 3., 12.])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. 使 $f(x) = \\sin(x)$，绘制 $f(x)$ 和 $\\frac{df(x)}{dx}$ 的图像，其中后者不使用 $f'(x) = \\cos(x)$。\n",
    "\n",
    "这个练习的目标是展示如何利用Autograd来**自动计算**一个函数的导数，而无需我们手动推导其解析形式。\n",
    "\n",
    "**实现思路**:\n",
    "1.  创建一个需要计算梯度的`x`张量，它应该包含一系列点，以便我们绘图。\n",
    "2.  计算 $y = \\sin(x)$。\n",
    "3.  因为 `y` 是一个向量，我们调用 `y.sum().backward()` 来计算每个点上的梯度。\n",
    "4.  梯度值会自动存储在 `x.grad` 中。"
   ],
   "id": "7b60bd5a95a3f379"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:09:05.252844Z",
     "start_time": "2025-07-24T15:09:05.248842Z"
    }
   },
   "cell_type": "code",
   "source": "from d2l import torch as d2l\n",
   "id": "a3a0d4c4ff49cba7",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:09:05.773622Z",
     "start_time": "2025-07-24T15:09:05.766622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(-5,5,0.1,dtype=torch.float32,requires_grad=True)\n",
    "x"
   ],
   "id": "b94f98cea8c6ab03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.0000e+00, -4.9000e+00, -4.8000e+00, -4.7000e+00, -4.6000e+00,\n",
       "        -4.5000e+00, -4.4000e+00, -4.3000e+00, -4.2000e+00, -4.1000e+00,\n",
       "        -4.0000e+00, -3.9000e+00, -3.8000e+00, -3.7000e+00, -3.6000e+00,\n",
       "        -3.5000e+00, -3.4000e+00, -3.3000e+00, -3.2000e+00, -3.1000e+00,\n",
       "        -3.0000e+00, -2.9000e+00, -2.8000e+00, -2.7000e+00, -2.6000e+00,\n",
       "        -2.5000e+00, -2.4000e+00, -2.3000e+00, -2.2000e+00, -2.1000e+00,\n",
       "        -2.0000e+00, -1.9000e+00, -1.8000e+00, -1.7000e+00, -1.6000e+00,\n",
       "        -1.5000e+00, -1.4000e+00, -1.3000e+00, -1.2000e+00, -1.1000e+00,\n",
       "        -1.0000e+00, -9.0000e-01, -8.0000e-01, -7.0000e-01, -6.0000e-01,\n",
       "        -5.0000e-01, -4.0000e-01, -3.0000e-01, -2.0000e-01, -1.0000e-01,\n",
       "        -2.9802e-09,  1.0000e-01,  2.0000e-01,  3.0000e-01,  4.0000e-01,\n",
       "         5.0000e-01,  6.0000e-01,  7.0000e-01,  8.0000e-01,  9.0000e-01,\n",
       "         1.0000e+00,  1.1000e+00,  1.2000e+00,  1.3000e+00,  1.4000e+00,\n",
       "         1.5000e+00,  1.6000e+00,  1.7000e+00,  1.8000e+00,  1.9000e+00,\n",
       "         2.0000e+00,  2.1000e+00,  2.2000e+00,  2.3000e+00,  2.4000e+00,\n",
       "         2.5000e+00,  2.6000e+00,  2.7000e+00,  2.8000e+00,  2.9000e+00,\n",
       "         3.0000e+00,  3.1000e+00,  3.2000e+00,  3.3000e+00,  3.4000e+00,\n",
       "         3.5000e+00,  3.6000e+00,  3.7000e+00,  3.8000e+00,  3.9000e+00,\n",
       "         4.0000e+00,  4.1000e+00,  4.2000e+00,  4.3000e+00,  4.4000e+00,\n",
       "         4.5000e+00,  4.6000e+00,  4.7000e+00,  4.8000e+00,  4.9000e+00],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:09:06.668717Z",
     "start_time": "2025-07-24T15:09:06.664812Z"
    }
   },
   "cell_type": "code",
   "source": "y = torch.sin(x)",
   "id": "6bf4489ff357305d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:09:07.073993Z",
     "start_time": "2025-07-24T15:09:07.069557Z"
    }
   },
   "cell_type": "code",
   "source": "y.sum().backward()",
   "id": "479a3a149fb47dd4",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T15:09:09.413832Z",
     "start_time": "2025-07-24T15:09:09.328980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d2l.plot(x.detach().numpy(), \n",
    "     [y.detach().numpy(), x.grad.detach().numpy()], \n",
    "     'x', 'f(x)', \n",
    "     legend=['sin(x)', 'grad of sin(x)'])#使用 .detach().numpy() 将张量转换为matplotlib可识别的numpy数组"
   ],
   "id": "95b3d76c836a9ee4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"254.660937pt\" height=\"183.35625pt\" viewBox=\"0 0 254.660937 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-07-24T23:09:09.392499</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 254.660937 183.35625 \nL 254.660937 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.160938 145.8 \nL 247.460938 145.8 \nL 247.460938 7.2 \nL 52.160938 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 78.972094 145.8 \nL 78.972094 7.2 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m161b1be5e4\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m161b1be5e4\" x=\"78.972094\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <g transform=\"translate(71.601001 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(83.789062 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 114.839863 145.8 \nL 114.839863 7.2 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m161b1be5e4\" x=\"114.839863\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <g transform=\"translate(107.468769 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(83.789062 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 150.707631 145.8 \nL 150.707631 7.2 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m161b1be5e4\" x=\"150.707631\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <g transform=\"translate(147.526381 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 186.575399 145.8 \nL 186.575399 7.2 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m161b1be5e4\" x=\"186.575399\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g transform=\"translate(183.394149 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 222.443167 145.8 \nL 222.443167 7.2 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m161b1be5e4\" x=\"222.443167\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(219.261917 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- x -->\n     <g transform=\"translate(146.851563 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 52.160938 139.504837 \nL 247.460938 139.504837 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"m4be33b9dbb\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m4be33b9dbb\" x=\"52.160938\" y=\"139.504837\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −1.0 -->\n      <g transform=\"translate(20.878125 143.304055) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(179.199219 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 52.160938 108.003628 \nL 247.460938 108.003628 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m4be33b9dbb\" x=\"52.160938\" y=\"108.003628\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −0.5 -->\n      <g transform=\"translate(20.878125 111.802846) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(83.789062 0)\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(147.412109 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(179.199219 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 52.160938 76.502418 \nL 247.460938 76.502418 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m4be33b9dbb\" x=\"52.160938\" y=\"76.502418\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(29.257812 80.301637) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 52.160938 45.001209 \nL 247.460938 45.001209 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m4be33b9dbb\" x=\"52.160938\" y=\"45.001209\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.5 -->\n      <g transform=\"translate(29.257812 48.800428) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 52.160938 13.5 \nL 247.460938 13.5 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m4be33b9dbb\" x=\"52.160938\" y=\"13.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(29.257812 17.299219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798438 85.121094) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(35.205078 0)\"/>\n      <use xlink:href=\"#DejaVuSans-78\" transform=\"translate(74.21875 0)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(133.398438 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 61.03821 16.087869 \nL 62.831597 14.60553 \nL 64.624984 13.741642 \nL 66.418379 13.504837 \nL 68.211766 13.897484 \nL 70.005152 14.915657 \nL 71.798539 16.549185 \nL 73.591926 18.781744 \nL 75.385321 21.591044 \nL 77.178708 24.948985 \nL 78.972099 28.822042 \nL 80.765485 33.171492 \nL 82.558876 37.953907 \nL 84.352263 43.121469 \nL 86.14565 48.622566 \nL 87.939041 54.40224 \nL 89.732423 60.402704 \nL 91.52581 66.564046 \nL 93.319201 72.824703 \nL 95.112587 79.122092 \nL 96.905978 85.39332 \nL 98.699365 91.575699 \nL 100.492752 97.60747 \nL 102.286143 103.428382 \nL 104.079534 108.980256 \nL 105.872921 114.207609 \nL 107.666312 119.058237 \nL 109.459698 123.483652 \nL 111.253089 127.439655 \nL 113.046476 130.8867 \nL 114.839865 133.790358 \nL 116.633254 136.121616 \nL 118.42664 137.857174 \nL 120.220029 138.979701 \nL 122.013418 139.477972 \nL 123.806805 139.347015 \nL 125.600194 138.588135 \nL 127.393582 137.208915 \nL 129.186971 135.223135 \nL 130.98036 132.650636 \nL 132.773747 129.517124 \nL 134.567136 125.853905 \nL 136.360523 121.697587 \nL 138.153912 117.08969 \nL 139.9473 112.076261 \nL 141.740689 106.707387 \nL 143.534077 101.036717 \nL 145.327465 95.120907 \nL 147.120854 89.019067 \nL 148.914242 82.792165 \nL 150.707631 76.502419 \nL 152.501019 70.212672 \nL 154.294408 63.98577 \nL 156.087796 57.88393 \nL 157.881185 51.96812 \nL 159.674573 46.297449 \nL 161.467962 40.928576 \nL 163.261351 35.915143 \nL 165.054738 31.30725 \nL 166.848127 27.150928 \nL 168.641515 23.487713 \nL 170.434904 20.354197 \nL 172.228293 17.781702 \nL 174.021681 15.795922 \nL 175.815068 14.416702 \nL 177.608457 13.657821 \nL 179.401846 13.526865 \nL 181.195233 14.025136 \nL 182.988621 15.147663 \nL 184.78201 16.883224 \nL 186.575399 19.214483 \nL 188.368786 22.118137 \nL 190.162177 25.565193 \nL 191.955563 29.521184 \nL 193.748954 33.946611 \nL 195.542341 38.797228 \nL 197.335732 44.024592 \nL 199.129119 49.576454 \nL 200.922506 55.397351 \nL 202.715897 61.429137 \nL 204.509283 67.611517 \nL 206.30267 73.88273 \nL 208.096061 80.180134 \nL 209.889448 86.440775 \nL 211.682839 92.602132 \nL 213.476225 98.60261 \nL 215.269612 104.382271 \nL 217.063003 109.883379 \nL 218.85639 115.050941 \nL 220.649776 119.833344 \nL 222.443167 124.182806 \nL 224.236554 128.055852 \nL 226.029941 131.413792 \nL 227.823336 134.223092 \nL 229.616723 136.455652 \nL 231.410109 138.08918 \nL 233.203496 139.107352 \nL 234.996883 139.5 \nL 236.790278 139.263195 \nL 238.583665 138.399306 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 61.03821 58.631014 \nL 62.831597 64.751682 \nL 64.624984 70.989759 \nL 66.418379 77.282946 \nL 68.211766 83.568305 \nL 70.005152 89.783064 \nL 71.798539 95.865127 \nL 73.591926 101.753725 \nL 75.385321 107.390046 \nL 77.178708 112.717723 \nL 78.972099 117.683558 \nL 80.765485 122.237916 \nL 82.558876 126.335308 \nL 84.352263 129.934777 \nL 86.14565 133.000369 \nL 87.939041 135.501461 \nL 89.732423 137.413042 \nL 91.52581 138.71603 \nL 93.319201 139.397403 \nL 95.112587 139.450348 \nL 96.905978 138.87434 \nL 98.699365 137.675131 \nL 100.492752 135.864709 \nL 102.286143 133.461151 \nL 104.079534 130.488479 \nL 105.872921 126.976402 \nL 107.666312 122.96 \nL 109.459698 118.479416 \nL 111.253089 113.579403 \nL 113.046476 108.30894 \nL 114.839865 102.720668 \nL 116.633254 96.870434 \nL 118.42664 90.816697 \nL 120.220029 84.619929 \nL 122.013418 78.342053 \nL 123.806805 72.045804 \nL 125.600194 65.794076 \nL 127.393582 59.649342 \nL 129.186971 53.672999 \nL 130.98036 47.924761 \nL 132.773747 42.462065 \nL 134.567136 37.339486 \nL 136.360523 32.608211 \nL 138.153912 28.315509 \nL 139.9473 24.504278 \nL 141.740689 21.212595 \nL 143.534077 18.473349 \nL 145.327465 16.313908 \nL 147.120854 14.755853 \nL 148.914242 13.814749 \nL 150.707631 13.5 \nL 152.501019 13.814749 \nL 154.294408 14.755853 \nL 156.087796 16.313908 \nL 157.881185 18.473349 \nL 159.674573 21.212595 \nL 161.467962 24.504278 \nL 163.261351 28.315513 \nL 165.054738 32.608211 \nL 166.848127 37.33949 \nL 168.641515 42.462065 \nL 170.434904 47.924766 \nL 172.228293 53.673007 \nL 174.021681 59.649349 \nL 175.815068 65.794076 \nL 177.608457 72.045804 \nL 179.401846 78.34206 \nL 181.195233 84.619929 \nL 182.988621 90.816697 \nL 184.78201 96.870441 \nL 186.575399 102.720676 \nL 188.368786 108.30894 \nL 190.162177 113.579414 \nL 191.955563 118.479416 \nL 193.748954 122.960011 \nL 195.542341 126.976402 \nL 197.335732 130.488487 \nL 199.129119 133.461151 \nL 200.922506 135.864705 \nL 202.715897 137.675131 \nL 204.509283 138.87434 \nL 206.30267 139.450348 \nL 208.096061 139.397403 \nL 209.889448 138.716034 \nL 211.682839 137.413042 \nL 213.476225 135.501454 \nL 215.269612 133.000369 \nL 217.063003 129.93477 \nL 218.85639 126.3353 \nL 220.649776 122.237916 \nL 222.443167 117.683546 \nL 224.236554 112.717723 \nL 226.029941 107.390046 \nL 227.823336 101.753725 \nL 229.616723 95.865127 \nL 231.410109 89.783064 \nL 233.203496 83.568305 \nL 234.996883 77.282946 \nL 236.790278 70.989759 \nL 238.583665 64.751682 \n\" clip-path=\"url(#p530dd33e16)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 52.160938 145.8 \nL 52.160938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 247.460938 145.8 \nL 247.460938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 52.160937 145.8 \nL 247.460938 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 52.160937 7.2 \nL 247.460938 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 100.321875 92.678125 \nL 199.3 92.678125 \nQ 201.3 92.678125 201.3 90.678125 \nL 201.3 62.321875 \nQ 201.3 60.321875 199.3 60.321875 \nL 100.321875 60.321875 \nQ 98.321875 60.321875 98.321875 62.321875 \nL 98.321875 90.678125 \nQ 98.321875 92.678125 100.321875 92.678125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 102.321875 68.420313 \nL 112.321875 68.420313 \nL 122.321875 68.420313 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- sin(x) -->\n     <g transform=\"translate(130.321875 71.920313) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(52.099609 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(79.882812 0)\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(143.261719 0)\"/>\n      <use xlink:href=\"#DejaVuSans-78\" transform=\"translate(182.275391 0)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(241.455078 0)\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 102.321875 83.098438 \nL 112.321875 83.098438 \nL 122.321875 83.098438 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- grad of sin(x) -->\n     <g transform=\"translate(130.321875 86.598438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(63.476562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(104.589844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(165.869141 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(229.345703 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(261.132812 0)\"/>\n      <use xlink:href=\"#DejaVuSans-66\" transform=\"translate(322.314453 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(357.519531 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(389.306641 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(441.40625 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(469.189453 0)\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(532.568359 0)\"/>\n      <use xlink:href=\"#DejaVuSans-78\" transform=\"translate(571.582031 0)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(630.761719 0)\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p530dd33e16\">\n   <rect x=\"52.160938\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a84740d6ce3d7828"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
