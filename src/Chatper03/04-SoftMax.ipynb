{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "21be315ada349cc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SoftMax\n",
    "\n",
    "### 什么是Softmax函数？为什么需要它？\n",
    "\n",
    "在深度学习中，尤其是在处理**分类问题**时，我们经常需要模型输出一个概率分布，来表示一个样本属于各个类别的置信度。\n",
    "\n",
    "例如，一个图像分类模型需要判断一张图片是猫、是狗、还是老虎。模型的最后一层（通常是全连接层）会为每个类别输出一个实数分数（score），我们称之为`logits`。比如：\n",
    "\n",
    "*   猫：2.0\n",
    "*   狗：1.0\n",
    "*   老虎：0.1\n",
    "\n",
    "这些分数越高，代表模型认为该样本属于这个类别的可能性越大。但这些分数并不是概率，它们可以是任意实数，而且它们的和也不为1。\n",
    "\n",
    "我们的目标是把这些`logits`转换成一个真正的概率分布，这个分布需要满足两个核心条件：\n",
    "1.  **非负性**：所有类别的概率值都必须在 $$ 区间内。\n",
    "2.  **归一化**：所有类别的概率值之和必须等于1。\n",
    "\n",
    "`Softmax`函数正是为了实现这一转换而设计的。它是一种将向量（`logits`）转换为概率分布的绝佳方式。\n",
    "\n",
    "### Softmax函数的公式推导\n",
    "\n",
    "让我们一步步推导出Softmax的公式。这个推导过程非常直观，体现了数学构造的美感。\n",
    "\n",
    "假设我们的模型对一个样本输出了$K$个类别的`logits`，记作向量 $z = (z_1, z_2, \\dots, z_K)$。\n",
    "\n",
    "#### **第一步：保证非负性**\n",
    "\n",
    "我们需要将所有 $z_i$（无论正负）都映射到正数。在数学中，**指数函数** $f(x) = e^x$ 是一个完美的工具，它有以下优点：\n",
    "*   **严格为正**：对于任何实数输入 $x$，$e^x$ 的值总是大于0。\n",
    "*   **单调递增**：如果 $z_i > z_j$，那么 $e^{z_i} > e^{z_j}$。这保证了原始`logits`的相对大小关系在转换后得以保留。分数越高的类别，其转换后的值也越大。\n",
    "\n",
    "应用指数函数后，我们得到一个新的向量： $(e^{z_1}, e^{z_2}, \\dots, e^{z_K})$。现在，所有的值都是正数了。\n",
    "\n",
    "#### **第二步：保证归一化**\n",
    "\n",
    "现在我们有了一组正数，如何让它们的和等于1呢？最简单的方法就是**将每个数除以所有数的总和**。这正是标准的归一化技巧。\n",
    "\n",
    "我们计算所有指数化后的值的总和：\n",
    "$$\n",
    "S = \\sum_{j=1}^{K} e^{z_j}\n",
    "$$\n",
    "然后，将每个指数化后的值 $e^{z_i}$ 都除以这个总和 $S$，得到第 $i$ 个类别的最终概率 $\\hat{y}_i$：\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{S} = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "#### **Softmax公式**\n",
    "\n",
    "综合以上两步，我们就得到了**Softmax函数的最终形式**：\n",
    "$$\n",
    "\\text{softmax}(z)_i = \\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } i=1, \\dots, K\n",
    "$$\n",
    "这个公式接收一个`logits`向量 $z$ 作为输入，输出一个概率分布向量 $\\hat{y}$，其中每个元素 $\\hat{y}_i$ 都在 $(0, 1)$ 之间，并且所有元素的和为1。\n",
    "\n",
    "#### **为什么叫 \"Softmax\"？**\n",
    "\n",
    "`argmax` 函数会找到最大值的索引，并返回一个`one-hot`向量（例如 `[0, 1, 0]`）。这是一种“硬”选择（Hard Max）。\n",
    "\n",
    "`Softmax` 则是 `argmax` 的一个“软化”（soft）或平滑版本。它不会断然地只选择一个最大值，而是根据`logits`的大小分配相应的概率。当某个`logit` $z_i$ 远大于其他`logits`时，其对应的概率 $\\hat{y}_i$ 会非常接近1，而其他概率会非常接近0。从这个角度看，它在效果上逼近了`argmax`，但函数本身是平滑可微的，这对基于梯度下降的优化至关重要。\n",
    "\n",
    "### Softmax与损失函数：交叉熵损失\n",
    "\n",
    "有了概率输出 $\\hat{y}$，我们还需要一个**损失函数 (Loss Function)** 来衡量模型预测的概率分布与真实的标签分布之间的差距。对于分类问题，最常用的损失函数是**交叉熵损失 (Cross-Entropy Loss)**。\n",
    "\n",
    "#### **信息论基础**\n",
    "\n",
    "要理解交叉熵，需要简单回顾一下信息论的两个基本概念：\n",
    "1.  **信息量**：一个事件发生的概率越低，它所包含的信息量就越大。信息量的公式是 $I(x) = -\\log(P(x))$。\n",
    "2.  **熵 (Entropy)**：一个概率分布中，所有可能事件信息量的期望值，用来度量该分布的不确定性。熵越大，不确定性越高。\n",
    "\n",
    "#### **交叉熵 (Cross-Entropy)**\n",
    "\n",
    "交叉熵 $H(p, q)$ 用来衡量两个概率分布 $p$（真实分布）和 $q$（预测分布）之间的差异。其公式为：\n",
    "$$\n",
    "H(p, q) = - \\sum_{i=1}^{K} p(x_i) \\log(q(x_i))\n",
    "$$\n",
    "在分类任务中，$p$ 是真实的标签分布，通常是一个`one-hot`向量。例如，如果样本的真实类别是第 $c$ 类，那么 $p_c = 1$，而所有其他的 $p_i = 0$ ($i \\neq c$)。\n",
    "\n",
    "将这个特性代入交叉熵公式，求和符号中的 $K$ 项里只有一项因为 $p_c=1$ 而保留下来，其余项都因为 $p_i=0$ 而消失。因此，损失函数被极大地简化了：\n",
    "$$\n",
    "L = H(y, \\hat{y}) = - \\sum_{i=1}^{K} y_i \\log(\\hat{y}_i) = -y_c \\log(\\hat{y}_c) = - \\log(\\hat{y}_c)\n",
    "$$\n",
    "其中 $y$ 是`one-hot`真实标签，$\\hat{y}$ 是Softmax的输出概率，$c$ 是正确的类别索引。\n",
    "\n",
    "这个简化的形式也被称为**对数似然损失 (Log-Likelihood Loss)**，因为它的目标是最大化正确类别的对数概率（等价于最小化其负对数概率）。\n",
    "\n",
    "### Softmax与交叉熵损失的导数\n",
    "\n",
    "在模型训练中，我们需要计算损失函数对`logits` $z_i$ 的梯度，以便通过反向传播更新网络参数。神奇的是，`Softmax`层和`Cross-Entropy`损失的组合会产生一个极其简洁优美的梯度。\n",
    "\n",
    "损失函数 $L = -\\log(\\hat{y}_c) = -\\log\\left(\\frac{e^{z_c}}{\\sum_{j=1}^{K} e^{z_j}}\\right)$。\n",
    "\n",
    "我们对任意一个logit $z_i$ 求偏导 $\\frac{\\partial L}{\\partial z_i}$，这里需要分两种情况讨论：\n",
    "\n",
    "1.  **当 $i = c$ 时 (对正确类别的logit求导)**:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial z_c} = \\frac{\\partial}{\\partial z_c} \\left( -\\log(\\hat{y}_c) \\right) = - \\frac{1}{\\hat{y}_c} \\frac{\\partial \\hat{y}_c}{\\partial z_c}\n",
    "    $$\n",
    "    其中 $\\frac{\\partial \\hat{y}_c}{\\partial z_c} = \\frac{e^{z_c}(\\sum e^{z_j}) - e^{z_c}(e^{z_c})}{(\\sum e^{z_j})^2} = \\frac{e^{z_c}}{\\sum e^{z_j}} \\left(1 - \\frac{e^{z_c}}{\\sum e^{z_j}}\\right) = \\hat{y}_c(1-\\hat{y}_c)$\n",
    "    代入后得到：$\\frac{\\partial L}{\\partial z_c} = - \\frac{1}{\\hat{y}_c} (\\hat{y}_c(1-\\hat{y}_c)) = \\hat{y}_c - 1$\n",
    "\n",
    "2.  **当 $i \\neq c$ 时 (对错误类别的logit求导)**:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial z_i} = - \\frac{1}{\\hat{y}_c} \\frac{\\partial \\hat{y}_c}{\\partial z_i}\n",
    "    $$\n",
    "    其中 $\\frac{\\partial \\hat{y}_c}{\\partial z_i} = \\frac{0 \\cdot (\\sum e^{z_j}) - e^{z_c}(e^{z_i})}{(\\sum e^{z_j})^2} = - \\frac{e^{z_c}}{\\sum e^{z_j}} \\frac{e^{z_i}}{\\sum e^{z_j}} = -\\hat{y}_c \\hat{y}_i$\n",
    "    代入后得到：$\\frac{\\partial L}{\\partial z_i} = - \\frac{1}{\\hat{y}_c} (-\\hat{y}_c \\hat{y}_i) = \\hat{y}_i$\n",
    "\n",
    "将两种情况整合起来，梯度可以统一写成：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n",
    "$$\n",
    "因为当 $i=c$ 时, $y_c=1$, $\\hat{y}_c-1$。 当 $i \\neq c$ 时, $y_i=0$, $\\hat{y}_i - 0$。\n",
    "\n",
    "这个结果 **$\\hat{y}_i - y_i$** 非常直观：**梯度就是预测概率与真实概率之间的差异**。如果模型预测的概率 $\\hat{y}_i$ 偏高了（相对于真实值$y_i$），梯度为正，参数会朝着减小 $z_i$ 的方向更新；如果预测偏低了，梯度为负，参数会朝着增大 $z_i$ 的方向更新。这种简洁性是Softmax+Cross-Entropy组合成为分类问题黄金搭档的核心原因。\n",
    "\n",
    "### 代码实现与数值稳定性\n",
    "\n",
    "在实际编码中，直接计算 $e^{z_i}$ 可能会导致数值溢出问题。如果某个 $z_i$ 很大（例如 1000），$e^{1000}$ 会超出计算机浮点数能表示的范围，得到`inf`。\n",
    "\n",
    "#### **数值稳定技巧**\n",
    "\n",
    "为了解决这个问题，我们可以利用Softmax的一个重要性质：对`logits`向量 $z$ 中的所有元素减去一个相同的常数 $C$，其Softmax输出结果不变。\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i - C}}{\\sum_{j=1}^{K} e^{z_j - C}} = \\frac{e^{z_i} e^{-C}}{e^{-C} \\sum_{j=1}^{K} e^{z_j}} = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "通常，我们选择 $C = \\max(z_1, z_2, \\dots, z_K)$。这样处理后：\n",
    "*   向量中的最大值变为0，所以 $e^0=1$。\n",
    "*   所有其他元素都变为负数或0，所以 $e^{z_i-C}$ 的值在 $(0, 1]$ 区间内。\n",
    "*   这就有效避免了上溢出（`inf`），同时也缓解了下溢出（结果太小而变为0）的风险。\n",
    "\n",
    "#### **Numpy 实现**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    为数值稳定性优化的Softmax函数实现\n",
    "    \n",
    "    Args:\n",
    "        z (np.ndarray): logits向量，可以是一维或二维(批量)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 概率分布\n",
    "    \"\"\"\n",
    "    assert z.ndim in [1, 2]\n",
    "    \n",
    "    # 减去最大值以保证数值稳定性\n",
    "    if z.ndim == 2:\n",
    "        # 对每行（每个样本）独立操作\n",
    "        z_max = np.max(z, axis=1, keepdims=True)\n",
    "        e_z = np.exp(z - z_max)\n",
    "        sum_e_z = np.sum(e_z, axis=1, keepdims=True)\n",
    "    else:\n",
    "        z_max = np.max(z)\n",
    "        e_z = np.exp(z - z_max)\n",
    "        sum_e_z = np.sum(e_z)\n",
    "        \n",
    "    return e_z / sum_e_z\n",
    "\n",
    "# 示例\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probabilities = softmax(logits)\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Sum of probabilities:\", np.sum(probabilities))\n",
    "\n",
    "# 批量样本的矢量化处理\n",
    "batch_logits = np.array([\n",
    "    [2.0, 1.0, 0.1],   # 样本1\n",
    "    [0.5, 3.0, 1.5]    # 样本2\n",
    "])\n",
    "batch_probabilities = softmax(batch_logits)\n",
    "print(\"\\nBatch Logits:\\n\", batch_logits)\n",
    "print(\"Batch Probabilities:\\n\", batch_probabilities)\n",
    "\n",
    "```\n",
    "\n",
    "#### **PyTorch 实现**\n",
    "\n",
    "在`PyTorch`中，Softmax和Cross-Entropy Loss已经为我们封装好了，并且进行了高度优化。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Softmax运算\n",
    "softmax_layer = nn.Softmax(dim=1) # dim=1表示对每一行(样本)进行softmax\n",
    "\n",
    "# 模拟网络输出的logits (批量大小为2，类别数为3)\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1],\n",
    "    [0.5, 3.0, 1.5]\n",
    "])\n",
    "\n",
    "probabilities = softmax_layer(logits)\n",
    "print(\"PyTorch Softmax Probabilities:\\n\", probabilities)\n",
    "\n",
    "# --- 交叉熵损失 ---\n",
    "# 真实标签\n",
    "labels = torch.tensor([0, 1]) # 第一个样本是第0类，第二个样本是第1类\n",
    "\n",
    "# PyTorch的CrossEntropyLoss集成了Softmax和NLLLoss\n",
    "# 为了更高的数值稳定性和效率，它直接接收原始的logits作为输入\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 计算损失\n",
    "loss = loss_fn(logits, labels)\n",
    "print(\"\\nCross-Entropy Loss:\", loss.item())\n",
    "\n",
    "```\n",
    "\n",
    "> **非常重要**：在`PyTorch`中，`nn.CrossEntropyLoss`的输入是**原始的`logits`**，而不是经过`nn.Softmax`之后的结果。这是因为它内部已经高效地实现了`LogSoftmax`和负对数似然损失（`NLLLoss`），这样组合可以获得更好的数值稳定性。这是一个初学者常见的使用误区。\n",
    "\n",
    "\n",
    "# 习题\n",
    "### 1. 计算 softmax交叉熵损失$l(\\mathbf{y}, \\hat{\\mathbf{y}})$的二阶导数\n",
    "\n",
    "#### **问题**\n",
    "\n",
    "1.  计算softmax交叉熵损失$l(\\mathbf{y}, \\hat{\\mathbf{y}})$的二阶导数。\n",
    "2.  计算softmax($\\mathbf{o}$)给出的分布方差，并与上面计算的二阶导数匹配。\n",
    "\n",
    "---\n",
    "\n",
    "#### **解答**\n",
    "\n",
    "##### **1. 二阶导数 (Hessian 矩阵)**\n",
    "\n",
    "我们首先回顾一下相关的定义。对于一个有 $q$ 个类别的分类问题，我们有：\n",
    "*   模型的原始输出（logits）：$\\mathbf{o} = [o_1, o_2, \\dots, o_q]^T$\n",
    "*   Softmax 函数：$\\hat{y}_i = \\text{softmax}(o_i) = \\frac{\\exp(o_i)}{\\sum_{k=1}^q \\exp(o_k)}$\n",
    "*   真实标签 (one-hot 编码)：$\\mathbf{y} = [y_1, y_2, \\dots, y_q]^T$, 其中只有一个 $y_i=1$\n",
    "*   交叉熵损失：$l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^q y_i \\log \\hat{y}_i$\n",
    "\n",
    "我们首先需要计算损失 $l$ 关于 logits $\\mathbf{o}$ 的一阶导数（梯度）。对任意一个 $o_j$求导：\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial o_j} = \\sum_{i=1}^q \\frac{\\partial l}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial o_j}\n",
    "$$\n",
    "经过计算（这个步骤通常在上节课中推导过），我们得到一个非常简洁的结果：\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial o_j} = \\hat{y}_j - y_j\n",
    "$$\n",
    "现在，我们来计算二阶导数，即Hessian矩阵 $H$，它的元素是 $H_{jk} = \\frac{\\partial^2 l}{\\partial o_j \\partial o_k}$。我们将对一阶导数 $\\hat{y}_j - y_j$ 再次关于 $o_k$ 求导。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 l}{\\partial o_j \\partial o_k} = \\frac{\\partial}{\\partial o_k}(\\hat{y}_j - y_j) = \\frac{\\partial \\hat{y}_j}{\\partial o_k}\n",
    "$$\n",
    "我们需要计算 $\\frac{\\partial \\hat{y}_j}{\\partial o_k}$。这里需要分两种情况讨论：\n",
    "\n",
    "*   **情况 1: $j=k$ (对角线元素)**\n",
    "    使用商法则对 $\\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_{i} \\exp(o_i)}$ 求导：\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{\\partial \\hat{y}_j}{\\partial o_j} & = \\frac{\\exp(o_j) \\left(\\sum_i \\exp(o_i)\\right) - \\exp(o_j) \\exp(o_j)}{\\left(\\sum_i \\exp(o_i)\\right)^2} \\\\\n",
    "    & = \\frac{\\exp(o_j)}{\\sum_i \\exp(o_i)} - \\left(\\frac{\\exp(o_j)}{\\sum_i \\exp(o_i)}\\right)^2 \\\\\n",
    "    & = \\hat{y}_j - \\hat{y}_j^2 = \\hat{y}_j(1-\\hat{y}_j)\n",
    "    \\end{align}\n",
    "    $$\n",
    "*   **情况 2: $j \\neq k$ (非对角线元素)**\n",
    "    同样使用商法则：\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{\\partial \\hat{y}_j}{\\partial o_k} & = \\frac{0 \\cdot \\left(\\sum_i \\exp(o_i)\\right) - \\exp(o_j) \\exp(o_k)}{\\left(\\sum_i \\exp(o_i)\\right)^2} \\\\\n",
    "    & = - \\frac{\\exp(o_j)}{\\sum_i \\exp(o_i)} \\frac{\\exp(o_k)}{\\sum_i \\exp(o_i)} \\\\\n",
    "    & = -\\hat{y}_j \\hat{y}_k\n",
    "    \\end{align}\n",
    "    $$\n",
    "所以，损失函数的Hessian矩阵 $H$ 为：\n",
    "$$\n",
    "H_{jk} =\n",
    "\\begin{cases}\n",
    "\\hat{y}_j(1-\\hat{y}_j) & \\text{if } j=k \\\\\n",
    "-\\hat{y}_j \\hat{y}_k & \\text{if } j \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "这个结果可以用矩阵形式优雅地表示为：\n",
    "$$\n",
    "H = \\text{diag}(\\hat{\\mathbf{y}}) - \\hat{\\mathbf{y}}\\hat{\\mathbf{y}}^T\n",
    "$$\n",
    "其中 $\\text{diag}(\\hat{\\mathbf{y}})$ 是一个对角矩阵，对角线上的元素是 $\\hat{y}_1, \\dots, \\hat{y}_q$。\n",
    "\n",
    "##### **2. 分布的方差**\n",
    "\n",
    "Softmax的输出 $\\hat{\\mathbf{y}}$ 定义了一个**范畴分布**（Categorical Distribution）。我们可以把一次抽样看作一个随机向量 $\\mathbf{Z}$，它以概率 $\\hat{y}_i$ 取值为标准基向量 $\\mathbf{e}_i$（即第$i$个位置为1，其余为0的向量）。\n",
    "\n",
    "这个分布的协方差矩阵（Covariance Matrix）定义为 $\\text{Cov}(\\mathbf{Z}) = E[\\mathbf{Z}\\mathbf{Z}^T] - E[\\mathbf{Z}]E[\\mathbf{Z}]^T$。\n",
    "\n",
    "*   **计算期望 $E[\\mathbf{Z}]$:**\n",
    "    $$\n",
    "    E[\\mathbf{Z}] = \\sum_{i=1}^q \\hat{y}_i \\mathbf{e}_i = [\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_q]^T = \\hat{\\mathbf{y}}\n",
    "    $$\n",
    "*   **计算 $E[\\mathbf{Z}\\mathbf{Z}^T]$:**\n",
    "    $\\mathbf{Z}\\mathbf{Z}^T$ 是一个 $q \\times q$ 的矩阵。\n",
    "    *   当 $j=k$ 时，矩阵的对角元素 $(\\mathbf{Z}\\mathbf{Z}^T)_{jj} = Z_j^2$。因为 $Z_j$ 只能取0或1，所以 $Z_j^2 = Z_j$。因此，$E[Z_j^2] = E[Z_j] = \\hat{y}_j$。\n",
    "    *   当 $j \\neq k$ 时，非对角元素 $(\\mathbf{Z}\\mathbf{Z}^T)_{jk} = Z_j Z_k$。由于一次抽样中只有一个元素为1，所以 $Z_j$ 和 $Z_k$ 不可能同时为1。因此 $Z_j Z_k = 0$。所以，$E[Z_j Z_k] = 0$。\n",
    "\n",
    "    将这些结果组合起来，$E[\\mathbf{Z}\\mathbf{Z}^T]$ 是一个对角矩阵，其对角线元素为 $\\hat{\\mathbf{y}}$。\n",
    "    $$\n",
    "    E[\\mathbf{Z}\\mathbf{Z}^T] = \\text{diag}(\\hat{\\mathbf{y}})\n",
    "    $$\n",
    "*   **计算协方差矩阵:**\n",
    "    $$\n",
    "    \\text{Cov}(\\mathbf{Z}) = E[\\mathbf{Z}\\mathbf{Z}^T] - E[\\mathbf{Z}]E[\\mathbf{Z}]^T = \\text{diag}(\\hat{\\mathbf{y}}) - \\hat{\\mathbf{y}}\\hat{\\mathbf{y}}^T\n",
    "    $$\n",
    "\n",
    "**匹配结果：**\n",
    "我们发现，**Softmax交叉熵损失函数的Hessian矩阵与Softmax函数所代表的范畴分布的协方差矩阵完全相同**。这在机器学习中是一个深刻且优美的结果，它将损失函数的曲率（二阶导数）与模型输出的内在不确定性（方差）联系了起来。在信息几何领域，这个矩阵也被称为**费雪信息矩阵**（Fisher Information Matrix）。\n",
    "\n",
    "### 2. 关于编码的问题\n",
    "\n",
    "#### **问题**\n",
    "假设我们有三个类发生的概率相等，即概率向量是$(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n",
    "1.  如果我们尝试为它设计二进制代码，有什么问题？\n",
    "2.  请设计一个更好的代码。\n",
    "\n",
    "---\n",
    "\n",
    "#### **解答**\n",
    "\n",
    "##### **1. 二进制编码的问题**\n",
    "为了给3个类别进行编码，我们最少需要 $\\lceil \\log_2(3) \\rceil = 2$ 个比特位。例如，我们可以设计如下编码：\n",
    "*   类别 1: `(0, 1)`\n",
    "*   类别 2: `(1, 0)`\n",
    "*   类别 3: `(1, 1)`\n",
    "\n",
    "如果我们使用这种编码作为神经网络的训练目标，并让网络输出一个2维向量，然后对每个维度使用Sigmoid激活函数来预测概率，会产生以下几个严重问题：\n",
    "\n",
    "*   **强加了不存在的结构关系**：这种编码方式在类别之间引入了人为的几何结构。例如，它暗示了类别之间的距离不同（例如，`(0,1)`和`(1,0)`的汉明距离是2，而`(0,1)`和`(1,1)`的汉明距离是1）。这迫使模型去学习一种并不存在的、错误的类别关系。\n",
    "*   **无法处理互斥性**：类别应该是互斥的（一个样本只能属于一个类别）。但使用两个独立的Sigmoid输出，模型被训练成两个独立的二元分类器。它没有被告知输出`(0, 0)`是无效的，也没有被告知两个输出是相关的。模型可能会预测出无效组合的概率，并且无法保证所有有效类别的概率之和为1。\n",
    "*   **损失函数不合理**：为每个输出使用二元交叉熵损失，意味着我们假设这两个比特位的预测是相互独立的。然而，它们共同决定一个单一的、不可分割的类别标签，因此它们不是独立的。\n",
    "\n",
    "##### **2. 更好的编码设计**\n",
    "在机器学习分类任务中，针对互斥类别的标准且最优的编码方式是**独热编码 (One-Hot Encoding)**。\n",
    "\n",
    "对于3个类别，我们的编码如下：\n",
    "*   类别 1: `(1, 0, 0)`\n",
    "*   类别 2: `(0, 1, 0)`\n",
    "*   类别 3: `(0, 0, 1)`\n",
    "\n",
    "这种编码方式与Softmax激活函数和交叉熵损失函数完美配合，具有以下优点：\n",
    "*   **类别等距**：在表示空间中，任何两个类别向量之间的欧氏距离和余弦相似度都是相同的。这正确地反映了在没有先验知识的情况下，类别之间没有顺序或远近关系。\n",
    "*   **保留互斥性**：结合Softmax函数，模型输出的向量 $(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3)$ 的所有元素之和为1，这天然地建模了类别之间的互斥性。\n",
    "*   **概率解释清晰**：Softmax的每个输出 $\\hat{y}_i$ 都直接对应于模型预测该样本属于类别 $i$ 的概率。\n",
    "\n",
    "*对提示的解读*：提示中提到的“编码两个独立的观察”和“联合编码n个观察”是在引导我们从信息论的角度思考。一个观察的熵是 $\\log_2 3 \\approx 1.58$ 比特。用2个比特编码单个观察是低效的。通过将 $n$ 个观察打包编码，平均每个观察所需的比特数可以趋近于熵，从而达到信息论的极限。然而，在设计神经网络的**目标表示 (target representation)** 时，我们的首要目标不是压缩效率，而是为梯度下降提供一个结构良好、易于学习的优化目标。因此，尽管在信息论上存在更高效的压缩编码，**One-Hot编码**仍然是分类任务中“更好”的、事实上的标准编码方案。\n",
    "\n",
    "### 3. 关于 RealSoftMax\n",
    "\n",
    "#### **问题**\n",
    "定义 $\\text{RealSoftMax}(a, b) = \\log(\\exp(a) + \\exp(b))$。\n",
    "1.  证明 $\\text{RealSoftMax}(a, b) > \\max(a, b)$。\n",
    "2.  证明 $\\lambda^{-1} \\text{RealSoftMax}(\\lambda a, \\lambda b) > \\max(a, b)$ 成立，前提是 $\\lambda > 0$。\n",
    "3.  证明对于 $\\lambda \\to \\infty$，有 $\\lambda^{-1} \\text{RealSoftMax}(\\lambda a, \\lambda b) \\to \\max(a, b)$。\n",
    "4.  soft-min会是什么样子？\n",
    "5.  将其扩展到两个以上的数字。\n",
    "\n",
    "---\n",
    "\n",
    "#### **解答**\n",
    "\n",
    "##### **1. 证明 $\\text{RealSoftMax}(a, b) > \\max(a, b)$**\n",
    "\n",
    "不失一般性，我们假设 $a \\ge b$，因此 $\\max(a, b) = a$。\n",
    "我们需要证明 $\\log(\\exp(a) + \\exp(b)) > a$。\n",
    "由于对数函数是单调递增的，我们可以对两边取指数运算，不等号方向不变：\n",
    "$$\n",
    "\\exp(\\log(\\exp(a) + \\exp(b))) > \\exp(a)\n",
    "$$\n",
    "$$\n",
    "\\exp(a) + \\exp(b) > \\exp(a)\n",
    "$$\n",
    "因为 $b$ 是一个实数，所以 $\\exp(b)$ 是一个严格大于0的数。因此，上式 $\\exp(a) + \\exp(b) > \\exp(a)$ 恒成立。\n",
    "证明完毕。\n",
    "\n",
    "##### **2. 证明 $\\lambda^{-1} \\text{RealSoftMax}(\\lambda a, \\lambda b) > \\max(a, b)$**\n",
    "根据第一部分的结论，对于任意输入 $x, y$，都有 $\\text{RealSoftMax}(x, y) > \\max(x, y)$。\n",
    "我们令 $x = \\lambda a$，$y = \\lambda b$。则有：\n",
    "$$\n",
    "\\text{RealSoftMax}(\\lambda a, \\lambda b) > \\max(\\lambda a, \\lambda b)\n",
    "$$\n",
    "因为 $\\lambda > 0$，所以 $\\max(\\lambda a, \\lambda b) = \\lambda \\max(a, b)$。\n",
    "代入上式得到：\n",
    "$$\n",
    "\\text{RealSoftMax}(\\lambda a, \\lambda b) > \\lambda \\max(a, b)\n",
    "$$\n",
    "由于 $\\lambda > 0$，两边同除以 $\\lambda$，不等号方向不变：\n",
    "$$\n",
    "\\lambda^{-1} \\text{RealSoftMax}(\\lambda a, \\lambda b) > \\max(a, b)\n",
    "$$\n",
    "证明完毕。\n",
    "\n",
    "##### **3. 证明 $\\lambda \\to \\infty$ 时的极限**\n",
    "我们需要计算极限 $\\lim_{\\lambda \\to \\infty} \\lambda^{-1} \\log(\\exp(\\lambda a) + \\exp(\\lambda b))$。\n",
    "不失一般性，我们再次假设 $a \\ge b$。\n",
    "*   如果 $a=b$，表达式变为 $\\lambda^{-1} \\log(2\\exp(\\lambda a)) = \\lambda^{-1}(\\log 2 + \\lambda a) = \\frac{\\log 2}{\\lambda} + a$。当 $\\lambda \\to \\infty$ 时，此式极限为 $a$，即 $\\max(a, a)$。\n",
    "*   如果 $a > b$，我们从对数中提出较大的项 $\\exp(\\lambda a)$：\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\quad \\lambda^{-1} \\log(\\exp(\\lambda a) (1 + \\frac{\\exp(\\lambda b)}{\\exp(\\lambda a)})) \\\\\n",
    "= & \\quad \\lambda^{-1} \\log(\\exp(\\lambda a) (1 + \\exp(\\lambda(b-a)))) \\\\\n",
    "= & \\quad \\lambda^{-1} [\\log(\\exp(\\lambda a)) + \\log(1 + \\exp(\\lambda(b-a)))] \\\\\n",
    "= & \\quad \\lambda^{-1} [\\lambda a + \\log(1 + \\exp(\\lambda(b-a)))] \\\\\n",
    "= & \\quad a + \\frac{\\log(1 + \\exp(\\lambda(b-a)))}{\\lambda}\n",
    "\\end{align}\n",
    "$$\n",
    "现在我们来考察第二项的极限。因为 $a > b$，所以 $b-a < 0$。当 $\\lambda \\to \\infty$ 时：\n",
    "*   $\\lambda(b-a) \\to -\\infty$\n",
    "*   $\\exp(\\lambda(b-a)) \\to 0$\n",
    "*   $1 + \\exp(\\lambda(b-a)) \\to 1$\n",
    "*   $\\log(1 + \\exp(\\lambda(b-a))) \\to \\log(1) = 0$\n",
    "所以，第二项的极限是 $\\frac{0}{\\infty}$ 的形式，极限为0。\n",
    "因此，整个表达式的极限是 $a + 0 = a$。\n",
    "因为我们假设了 $\\max(a, b) = a$，所以极限等于 $\\max(a, b)$。\n",
    "证明完毕。这个结果表明，`RealSoftMax` 是 `max` 函数的一个平滑近似，参数 $\\lambda$ 控制了近似的“硬度”，$\\lambda$ 越大，近似越精确。\n",
    "\n",
    "##### **4. Soft-min 的形式**\n",
    "`min` 和 `max` 函数之间存在一个对偶关系：$\\min(a, b) = - \\max(-a, -b)$。\n",
    "我们可以利用这个关系来定义 `soft-min`：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{soft-min}(a, b) & = - \\text{RealSoftMax}(-a, -b) \\\\\n",
    "& = - \\log(\\exp(-a) + \\exp(-b))\n",
    "\\end{align}\n",
    "$$\n",
    "这就是 `soft-min` 的形式。它同样是 `min` 函数的一个平滑近似。\n",
    "\n",
    "##### **5. 扩展到多个数字**\n",
    "将 `RealSoftMax` 和 `soft-min` 扩展到 $n$ 个数字是很自然的：\n",
    "*   **RealSoftMax (Log-Sum-Exp)**:\n",
    "    $\\text{RealSoftMax}(x_1, \\dots, x_n) = \\log(\\exp(x_1) + \\dots + \\exp(x_n)) = \\log\\left(\\sum_{i=1}^n \\exp(x_i)\\right)$。\n",
    "    这个函数在机器学习中非常重要，通常被称为**Log-Sum-Exp**技巧，用于在对数域里进行概率计算，以避免数值下溢或上溢，例如在计算log-softmax时。\n",
    "\n",
    "*   **Soft-min**:\n",
    "    $\\text{soft-min}(x_1, \\dots, x_n) = - \\text{RealSoftMax}(-x_1, \\dots, -x_n) = - \\log\\left(\\sum_{i=1}^n \\exp(-x_i)\\right)$。"
   ],
   "id": "b1dbabb1c7571e92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9d732cbff9b0fd97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
