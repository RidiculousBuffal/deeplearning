{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 线性回归\n",
    "### 线性回归是什么？\n",
    "\n",
    "线性回归（Linear Regression）是监督学习中最基础和最经典的算法之一。它的核心目标是找到一个线性模型，这个模型能够尽可能准确地预测一个或多个自变量（特征）与一个连续的因变量（目标值）之间的关系。\n",
    "\n",
    "简单来说，如果我们将数据点绘制在图上，线性回归就是试图找到一条直线（在二维空间）或一个平面/超平面（在更高维空间），使其最佳地拟合这些数据点。\n",
    "\n",
    "\n",
    "\n",
    "### 1. 模型的定义与公式\n",
    "\n",
    "#### a. 标量形式\n",
    "在具有 $d$ 个特征的数据集中，对于单个样本，线性模型可以表示为：\n",
    "\n",
    "$$\n",
    "\\widehat{y}=\\omega _1x_1+\\omega _2x_2+\\cdots \\omega _dx_d+b\n",
    "$$\n",
    "\n",
    "这里：\n",
    "*   $d$ 是特征的数量。\n",
    "*   $x_j$ 是第 $j$ 个特征的值。\n",
    "*   $\\omega_j$ 是第 $j$ 个特征对应的权重（weight），它表示该特征对预测结果的重要性。\n",
    "*   $b$ 是偏置项（bias）或截距（intercept），表示当所有特征都为0时，模型的基准预测值。\n",
    "*   $\\widehat{y}$ 是模型的预测值。\n",
    "\n",
    "#### b. 向量形式\n",
    "为了简化表示，我们可以将特征和权重分别写成向量形式。对于单个样本，其特征向量为 $\\mathbf{x} = [x_1, x_2, \\dots, x_d]^T$，权重向量为 $\\mathbf{\\omega} = [\\omega_1, \\omega_2, \\dots, \\omega_d]^T$。模型可以表示为：\n",
    "\n",
    "$$\n",
    "\\widehat{y}=\\mathbf{\\omega }^{\\mathrm{T}}\\mathbf{x}+b\n",
    "$$\n",
    "\n",
    "其中 $\\mathbf{\\omega }^{\\mathrm{T}}\\mathbf{x}$ 是两个向量的点积。\n",
    "\n",
    "#### c. 矩阵形式\n",
    "当处理整个数据集时（假设有 $n$ 个样本），使用矩阵表示法会更加高效。我们将整个数据集的特征表示为一个矩阵 $\\mathbf{X}$，其中每一行是一个样本，每一列是一个特征。\n",
    "\n",
    "*   **设计矩阵 (Design Matrix) $\\mathbf{X}$**:\n",
    "    $$\n",
    "    \\mathbf{X} = \\begin{pmatrix}\n",
    "     x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "     x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "     \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "     x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    这是一个 $n \\times d$ 的矩阵。\n",
    "\n",
    "*   **权重向量 (Weight Vector) $\\mathbf{\\omega}$**:\n",
    "    $$ \\mathbf{\\omega} = [\\omega_1, \\omega_2, \\dots, \\omega_d]^T $$\n",
    "    这是一个 $d \\times 1$ 的向量。\n",
    "\n",
    "*   **预测向量 (Prediction Vector) $\\widehat{\\mathbf{y}}$**:\n",
    "    $$ \\widehat{\\mathbf{y}} = [\\widehat{y}_1, \\widehat{y}_2, \\dots, \\widehat{y}_n]^T $$\n",
    "    这是一个 $n \\times 1$ 的向量，包含了对所有 $n$ 个样本的预测值。\n",
    "\n",
    "此时，模型的矩阵形式为：\n",
    "$$\n",
    "\\widehat{\\mathbf{y}}=\\mathbf{X\\omega }+b\n",
    "$$\n",
    "注意这里的 $b$ 是一个标量，它会被广播（broadcast）加到向量 $\\mathbf{X\\omega}$ 的每一个元素上。\n",
    "\n",
    "**一个简化技巧**：为了将偏置项 $b$ 也统一到矩阵乘法中，我们通常会对数据进行增广处理。具体做法是在特征矩阵 $\\mathbf{X}$ 中增加一列全为1的向量，同时在权重向量 $\\mathbf{\\omega}$ 中增加偏置项 $b$ 作为其最后一个元素。\n",
    "\n",
    "*   增广后的特征矩阵 $\\mathbf{X}'$ ($n \\times (d+1)$):\n",
    "    $$\n",
    "    \\mathbf{X}' = \\begin{pmatrix}\n",
    "     x_{11} & \\cdots & x_{1d} & 1 \\\\\n",
    "     x_{21} & \\cdots & x_{2d} & 1 \\\\\n",
    "     \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "     x_{n1} & \\cdots & x_{nd} & 1\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "*   增广后的权重向量 $\\mathbf{\\omega}'$ ($(d+1) \\times 1$):\n",
    "    $$ \\mathbf{\\omega}' = [\\omega_1, \\dots, \\omega_d, b]^T $$\n",
    "\n",
    "这样，模型就可以简化为：\n",
    "$$\n",
    "\\widehat{\\mathbf{y}} = \\mathbf{X}'\\mathbf{\\omega}'\n",
    "$$\n",
    "\n",
    "在接下来的推导中，为了书写方便，我们默认使用增广后的 $\\mathbf{X}$ 和 $\\mathbf{\\omega}$，模型即 $\\widehat{\\mathbf{y}} = \\mathbf{X\\omega}$。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 损失函数 (Loss Function)\n",
    "\n",
    "模型的好坏需要一个指标来衡量。这个指标就是损失函数，它量化了模型的预测值 $\\widehat{\\mathbf{y}}$ 与真实值 $\\mathbf{y}$之间的差距。对于回归问题，最常用的损失函数是**均方误差 (Mean Squared Error, MSE)**。\n",
    "\n",
    "对于整个包含 $n$ 个样本的数据集，总损失 $L(\\mathbf{\\omega})$ 定义为所有样本的预测误差平方和：\n",
    "$$\n",
    "L(\\mathbf{\\omega}) = \\sum_{i=1}^{n} (\\widehat{y}_i - y_i)^2\n",
    "$$\n",
    "其中 $y_i$ 是第 $i$ 个样本的真实值。为了在求导时简化计算，我们通常会乘以一个常数 $\\frac{1}{2}$ 或 $\\frac{1}{2n}$，这不影响最优解的位置。我们采用 $\\frac{1}{2}$ 的形式：\n",
    "$$\n",
    "L(\\mathbf{\\omega}) = \\frac{1}{2} \\sum_{i=1}^{n} (\\widehat{y}_i - y_i)^2\n",
    "$$\n",
    "使用矩阵形式表示，损失函数可以写成：\n",
    "$$\n",
    "L(\\mathbf{\\omega}) = \\frac{1}{2} || \\widehat{\\mathbf{y}} - \\mathbf{y} ||_2^2 = \\frac{1}{2} || \\mathbf{X\\omega} - \\mathbf{y} ||_2^2\n",
    "$$\n",
    "其中 $||\\cdot||_2^2$ 表示L2范数的平方，即向量中各元素平方和。这等价于：\n",
    "$$\n",
    "L(\\mathbf{\\omega}) = \\frac{1}{2} (\\mathbf{X\\omega} - \\mathbf{y})^T (\\mathbf{X\\omega} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 总体目标\n",
    "\n",
    "线性回归的总体目标非常明确：**找到一组最优的权重参数 $\\mathbf{\\omega}^*$，使得损失函数 $L(\\mathbf{\\omega})$ 的值最小**。\n",
    "\n",
    "用数学语言描述，就是求解一个最优化问题：\n",
    "$$\n",
    "\\mathbf{\\omega}^* = \\arg\\min_{\\mathbf{\\omega}} L(\\mathbf{\\omega}) = \\arg\\min_{\\mathbf{\\omega}} \\frac{1}{2} || \\mathbf{X\\omega} - \\mathbf{y} ||_2^2\n",
    "$$\n",
    "求解这个问题主要有两种方法：**解析解**和**数值解（如梯度下降）**。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 解析解 (Analytical Solution) 的推导\n",
    "\n",
    "解析解，也称为正规方程 (Normal Equation)，是一种通过纯粹的数学推导直接给出最优解公式的方法。\n",
    "\n",
    "**推导过程：**\n",
    "\n",
    "1.  **写出损失函数**：\n",
    "    $$\n",
    "    L(\\mathbf{\\omega}) = \\frac{1}{2} (\\mathbf{X\\omega} - \\mathbf{y})^T (\\mathbf{X\\omega} - \\mathbf{y})\n",
    "    $$\n",
    "\n",
    "2.  **展开表达式**：\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    L(\\mathbf{\\omega}) &= \\frac{1}{2} ((\\mathbf{X\\omega})^T - \\mathbf{y}^T) (\\mathbf{X\\omega} - \\mathbf{y}) \\\\\n",
    "    &= \\frac{1}{2} (\\mathbf{\\omega}^T\\mathbf{X}^T - \\mathbf{y}^T) (\\mathbf{X\\omega} - \\mathbf{y}) \\\\\n",
    "    &= \\frac{1}{2} (\\mathbf{\\omega}^T\\mathbf{X}^T\\mathbf{X\\omega} - \\mathbf{\\omega}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X\\omega} + \\mathbf{y}^T\\mathbf{y})\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    注意到 $\\mathbf{y}^T\\mathbf{X\\omega}$ 是一个标量，它的转置等于自身，即 $(\\mathbf{y}^T\\mathbf{X\\omega})^T = \\mathbf{\\omega}^T\\mathbf{X}^T\\mathbf{y}$。因此，中间两项是相等的。\n",
    "    $$\n",
    "    L(\\mathbf{\\omega}) = \\frac{1}{2} (\\mathbf{\\omega}^T\\mathbf{X}^T\\mathbf{X\\omega} - 2\\mathbf{\\omega}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{y}^T\\mathbf{y})\n",
    "    $$\n",
    "\n",
    "3.  **对 $\\mathbf{\\omega}$ 求梯度**：\n",
    "    为了找到最小值，我们需要对 $L(\\mathbf{\\omega})$ 关于 $\\mathbf{\\omega}$ 求梯度（导数），并令其为零。这里需要用到两个矩阵微积分的结论：\n",
    "    *   $\\frac{\\partial (\\mathbf{w}^T \\mathbf{A} \\mathbf{w})}{\\partial \\mathbf{w}} = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{w}$。当 $\\mathbf{A}$ 是对称矩阵时，等于 $2\\mathbf{A}\\mathbf{w}$。\n",
    "    *   $\\frac{\\partial (\\mathbf{a}^T \\mathbf{w})}{\\partial \\mathbf{w}} = \\frac{\\partial (\\mathbf{w}^T \\mathbf{a})}{\\partial \\mathbf{w}} = \\mathbf{a}$。\n",
    "\n",
    "    在我们的损失函数中，$\\mathbf{X}^T\\mathbf{X}$ 是一个对称矩阵。因此，求导结果为：\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\nabla_{\\mathbf{\\omega}}L(\\mathbf{\\omega}) &= \\frac{\\partial L(\\mathbf{\\omega})}{\\partial \\mathbf{\\omega}} \\\\\n",
    "    &= \\frac{1}{2} (2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\omega} - 2\\mathbf{X}^T\\mathbf{y} + 0) \\\\\n",
    "    &= \\mathbf{X}^T\\mathbf{X}\\mathbf{\\omega} - \\mathbf{X}^T\\mathbf{y}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "4.  **令梯度为零并求解**：\n",
    "    $$\n",
    "    \\mathbf{X}^T\\mathbf{X}\\mathbf{\\omega} - \\mathbf{X}^T\\mathbf{y} = 0\n",
    "    $$\n",
    "    移项可得：\n",
    "    $$\n",
    "    \\mathbf{X}^T\\mathbf{X}\\mathbf{\\omega} = \\mathbf{X}^T\\mathbf{y}\n",
    "    $$\n",
    "    最后，在 $\\mathbf{X}^T\\mathbf{X}$ 可逆的情况下，两边同乘以 $(\\mathbf{X}^T\\mathbf{X})^{-1}$，得到 $\\mathbf{\\omega}$ 的解析解：\n",
    "    $$\n",
    "    \\mathbf{\\omega}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "    $$\n",
    "    这就是著名的**正规方程 (Normal Equation)**。\n",
    "\n",
    "**解析解的优缺点**：\n",
    "*   **优点**：无需选择学习率，一次计算即可得到最优解。\n",
    "*   **缺点**：需要计算矩阵 $(\\mathbf{X}^T\\mathbf{X})$ 的逆。该矩阵的维度是 $(d+1) \\times (d+1)$。矩阵求逆的计算复杂度大约是 $O(d^3)$。当特征数量 $d$ 非常大时（例如上万甚至百万），计算成本会极其高昂，甚至不可行。 此外，如果 $\\mathbf{X}^T\\mathbf{X}$ 是奇异矩阵（不可逆），则无法使用此方法。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 梯度下降 (Gradient Descent)\n",
    "\n",
    "当解析解不可行时，我们可以使用迭代优化的方法，其中最常用的就是梯度下降。\n",
    "\n",
    "**核心思想**：\n",
    "梯度下降就像一个下山的过程。我们从参数空间的一个随机点（随机初始化 $\\mathbf{\\omega}$）开始，沿着当前位置**梯度最大**的**反方向**（即最陡峭的下坡方向）走一小步，然后在新位置重复此过程，直到走到山谷的最低点（即损失函数的最小值点）。\n",
    "\n",
    "**a. (批量)梯度下降 (Batch Gradient Descent)**\n",
    "\n",
    "批量梯度下降在每次更新参数时，都会使用**全部**的训练数据。\n",
    "\n",
    "1.  **梯度**：我们已经推导出损失函数关于 $\\mathbf{\\omega}$ 的梯度：\n",
    "    $$ \\nabla_{\\mathbf{\\omega}}L(\\mathbf{\\omega}) = \\mathbf{X}^T(\\mathbf{X\\omega} - \\mathbf{y}) $$\n",
    "\n",
    "2.  **更新规则**：在每一步迭代中，我们按照以下规则更新 $\\mathbf{\\omega}$：\n",
    "    $$ \\mathbf{\\omega} := \\mathbf{\\omega} - \\eta \\nabla_{\\mathbf{\\omega}}L(\\mathbf{\\omega}) $$\n",
    "    其中：\n",
    "    *   `:=` 表示更新操作。\n",
    "    *   $\\eta$ (eta) 是**学习率 (Learning Rate)**，它控制着每一步走的“步长”。学习率过大可能导致在最低点附近震荡甚至发散；学习率过小则会导致收敛速度过慢。\n",
    "\n",
    "**b. 随机梯度下降 (Stochastic Gradient Descent, SGD)**\n",
    "\n",
    "批量梯度下降的缺点是，当数据集非常大（$n$ 很大）时，每次计算梯度都需要遍历所有样本，计算开销巨大。随机梯度下降为此提供了解决方案。\n",
    "\n",
    "**核心思想**：\n",
    "SGD 在每次更新时，不再使用全部数据，而是**随机地**从数据集中选择**一个**样本 $(x_i, y_i)$ 来计算梯度并更新参数。\n",
    "\n",
    "1.  **单样本损失与梯度**：对于单个样本 $i$，损失为 $L_i(\\mathbf{\\omega}) = \\frac{1}{2}(\\widehat{y}_i - y_i)^2 = \\frac{1}{2}(\\mathbf{x}_i^T\\mathbf{\\omega} - y_i)^2$。（这里我们假设 $\\mathbf{x}_i$ 和 $\\mathbf{\\omega}$ 已经增广）\n",
    "    其梯度为：\n",
    "    $$ \\nabla_{\\mathbf{\\omega}}L_i(\\mathbf{\\omega}) = (\\mathbf{x}_i^T\\mathbf{\\omega} - y_i)\\mathbf{x}_i = (\\widehat{y}_i - y_i)\\mathbf{x}_i $$\n",
    "\n",
    "2.  **更新规则**：（在每个迭代轮次中，对于随机选到的样本 $i$）\n",
    "    $$ \\mathbf{\\omega} := \\mathbf{\\omega} - \\eta (\\widehat{y}_i - y_i)\\mathbf{x}_i $$\n",
    "\n",
    "**SGD的优缺点**：\n",
    "*   **优点**：每次更新速度极快，计算开销小。其随机性有时反而有助于跳出局部最优解，找到更好的全局最优解。非常适合大规模数据集和在线学习场景。\n",
    "*   **缺点**：由于每次只用一个样本，梯度估计的噪声很大，导致收敛过程非常不稳定，路径曲折。\n",
    "\n",
    "**c. 小批量梯度下降 (Mini-batch Gradient Descent)**\n",
    "\n",
    "这是对批量梯度下降和随机梯度下降的一种折中，也是目前实际应用中最广泛的方法。它在每次更新时，使用一小批（mini-batch，例如32、64、128个）随机样本来计算梯度。\n",
    "\n",
    "**更新规则**：（对于一个大小为 $m$ 的小批量 $B$）\n",
    "$$ \\mathbf{\\omega} := \\mathbf{\\omega} - \\eta \\frac{1}{m} \\sum_{i \\in B} (\\widehat{y}_i - y_i)\\mathbf{x}_i $$\n",
    "它兼具了BGD的稳定性和SGD的高效性，是深度学习等领域进行优化的标准做法。"
   ],
   "id": "4b7ac118663b7319"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 矢量加速",
   "id": "8c187161d933a025"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-26T12:24:56.684640Z",
     "start_time": "2025-07-26T12:24:50.142233Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T12:25:17.220728Z",
     "start_time": "2025-07-26T12:25:17.211730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = 10000\n",
    "a = torch.ones([n])\n",
    "b = torch.ones([n])\n",
    "a, b"
   ],
   "id": "c60563965701df04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T12:26:18.925795Z",
     "start_time": "2025-07-26T12:26:18.841281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c = torch.zeros(n)\n",
    "timer = d2l.Timer()\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "\n",
    "f'{timer.stop():.5f} sec'"
   ],
   "id": "672a1b931b2a4c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.08017 sec'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T12:26:45.884446Z",
     "start_time": "2025-07-26T12:26:45.879445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.17f} sec'"
   ],
   "id": "3f997f17b139d975",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000000000000 sec'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 正态分布和平方损失\n",
    "$$\n",
    "p\\left( x \\right) =\\frac{1}{\\sqrt{2\\pi \\sigma ^2}}e^{\\left( -\\frac{1}{2\\sigma ^2}\\left( x-\\mu \\right) ^2 \\right)}\n",
    "$$\n"
   ],
   "id": "74a097cd2f872176"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T12:33:05.819603Z",
     "start_time": "2025-07-26T12:33:05.716263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi*sigma**2)\n",
    "    return p * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "\n",
    "x = np.arange(-7, 7, 0.01)\n",
    "params = [(0, 1), (0, 2), (3, 1)]\n",
    "d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x', ylabel='p(x)', figsize=(4.5, 2.5),\n",
    "         legend=[f'mean {mu},std {sigma}' for mu, sigma in params])"
   ],
   "id": "b1ccb50de309253a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 450x250 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"302.08125pt\" height=\"183.35625pt\" viewBox=\"0 0 302.08125 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-07-26T20:33:05.794435</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 302.08125 183.35625 \nL 302.08125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 145.8 \nL 294.88125 145.8 \nL 294.88125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 71.511736 145.8 \nL 71.511736 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m018b411661\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"71.511736\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −6 -->\n      <g transform=\"translate(64.140642 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-36\" transform=\"translate(83.789062 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 104.145435 145.8 \nL 104.145435 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"104.145435\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −4 -->\n      <g transform=\"translate(96.774342 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(83.789062 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 136.779135 145.8 \nL 136.779135 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"136.779135\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −2 -->\n      <g transform=\"translate(129.408041 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(83.789062 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 169.412834 145.8 \nL 169.412834 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"169.412834\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(166.231584 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 202.046534 145.8 \nL 202.046534 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"202.046534\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2 -->\n      <g transform=\"translate(198.865284 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 234.680233 145.8 \nL 234.680233 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"234.680233\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 4 -->\n      <g transform=\"translate(231.498983 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path d=\"M 267.313932 145.8 \nL 267.313932 7.2 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m018b411661\" x=\"267.313932\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6 -->\n      <g transform=\"translate(264.132682 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- x -->\n     <g transform=\"translate(166.371875 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path d=\"M 43.78125 139.5 \nL 294.88125 139.5 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path id=\"m91cbfc4ffa\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m91cbfc4ffa\" x=\"43.78125\" y=\"139.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 143.299219) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path d=\"M 43.78125 107.916484 \nL 294.88125 107.916484 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m91cbfc4ffa\" x=\"43.78125\" y=\"107.916484\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.1 -->\n      <g transform=\"translate(20.878125 111.715702) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path d=\"M 43.78125 76.332967 \nL 294.88125 76.332967 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m91cbfc4ffa\" x=\"43.78125\" y=\"76.332967\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 80.132186) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path d=\"M 43.78125 44.749451 \nL 294.88125 44.749451 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m91cbfc4ffa\" x=\"43.78125\" y=\"44.749451\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.3 -->\n      <g transform=\"translate(20.878125 48.54867) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-33\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path d=\"M 43.78125 13.165935 \nL 294.88125 13.165935 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m91cbfc4ffa\" x=\"43.78125\" y=\"13.165935\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 16.965154) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- p(x) -->\n     <g transform=\"translate(14.798438 86.535156) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(63.476562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-78\" transform=\"translate(102.490234 0)\"/>\n      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(161.669922 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 55.194886 139.5 \nL 108.061479 139.392742 \nL 113.44604 139.148719 \nL 117.035747 138.770771 \nL 119.809611 138.25954 \nL 122.09397 137.619981 \nL 124.051992 136.856558 \nL 125.846846 135.932618 \nL 127.478531 134.864093 \nL 129.110215 133.53546 \nL 130.7419 131.902404 \nL 132.373585 129.918522 \nL 133.842102 127.79423 \nL 135.473787 125.01546 \nL 137.105472 121.755388 \nL 138.737157 117.977866 \nL 140.368842 113.655911 \nL 142.163695 108.255858 \nL 144.121717 101.596938 \nL 146.242908 93.525729 \nL 148.690435 83.248545 \nL 151.790637 69.178196 \nL 157.827871 41.57206 \nL 159.949061 33.006718 \nL 161.743915 26.675681 \nL 163.212431 22.276554 \nL 164.517779 19.044317 \nL 165.659959 16.789011 \nL 166.63897 15.307609 \nL 167.454812 14.403942 \nL 168.270655 13.808322 \nL 168.923329 13.556687 \nL 169.576003 13.5063 \nL 170.228677 13.657402 \nL 170.881351 14.009268 \nL 171.697193 14.728769 \nL 172.513036 15.753897 \nL 173.492047 17.376612 \nL 174.634226 19.788832 \nL 175.776406 22.726997 \nL 177.081754 26.675681 \nL 178.713439 32.392617 \nL 180.671461 40.191426 \nL 183.118988 50.957567 \nL 192.256424 92.210802 \nL 194.540783 101.006788 \nL 196.498805 107.731288 \nL 198.456827 113.655911 \nL 200.25168 118.379758 \nL 201.883365 122.104146 \nL 203.51505 125.314336 \nL 205.146735 128.047227 \nL 206.77842 130.345625 \nL 208.410105 132.255577 \nL 210.04179 133.824084 \nL 211.673475 135.097239 \nL 213.468328 136.208702 \nL 215.42635 137.136646 \nL 217.547541 137.875773 \nL 219.8319 138.435791 \nL 222.605764 138.879679 \nL 226.032303 139.193999 \nL 230.927358 139.396705 \nL 239.412119 139.487295 \nL 277.104042 139.5 \nL 283.467614 139.5 \nL 283.467614 139.5 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 55.194886 139.362188 \nL 65.63767 139.098708 \nL 72.980253 138.699787 \nL 78.69115 138.178279 \nL 83.586205 137.516853 \nL 87.991754 136.697191 \nL 91.907798 135.745999 \nL 95.497505 134.654637 \nL 98.924044 133.387507 \nL 102.187413 131.951607 \nL 105.450783 130.271147 \nL 108.714153 128.328848 \nL 111.977523 126.112322 \nL 115.240893 123.615644 \nL 118.504263 120.840886 \nL 121.930802 117.640793 \nL 125.683677 113.829699 \nL 130.089226 109.01843 \nL 136.126461 102.052696 \nL 144.448054 92.482389 \nL 148.364098 88.331596 \nL 151.627468 85.194678 \nL 154.401333 82.824913 \nL 157.012028 80.888277 \nL 159.296387 79.455574 \nL 161.580746 78.288522 \nL 163.701937 77.457339 \nL 165.659959 76.915213 \nL 167.617981 76.595215 \nL 169.576003 76.500787 \nL 171.534025 76.632947 \nL 173.492047 76.99027 \nL 175.450069 77.568916 \nL 177.571259 78.438306 \nL 179.69245 79.54932 \nL 181.976809 81.000265 \nL 184.424336 82.824913 \nL 187.035032 85.047175 \nL 189.972065 87.839952 \nL 193.398603 91.412502 \nL 197.804153 96.3499 \nL 212.978823 113.657453 \nL 216.894867 117.640793 \nL 220.484574 120.986092 \nL 223.911112 123.877929 \nL 227.174482 126.346542 \nL 230.437852 128.535259 \nL 233.701222 130.450724 \nL 236.964592 132.105883 \nL 240.227962 133.51842 \nL 243.6545 134.763387 \nL 247.244207 135.834289 \nL 251.160251 136.766392 \nL 255.402632 137.542786 \nL 260.134519 138.178279 \nL 265.682247 138.687887 \nL 272.372156 139.065638 \nL 281.020086 139.318239 \nL 283.467614 139.359757 \nL 283.467614 139.359757 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 55.194886 139.5 \nL 157.012028 139.392742 \nL 162.396589 139.148719 \nL 165.986296 138.770771 \nL 168.76016 138.25954 \nL 171.044519 137.619981 \nL 173.002541 136.856558 \nL 174.797395 135.932618 \nL 176.42908 134.864093 \nL 178.060765 133.53546 \nL 179.69245 131.902404 \nL 181.324135 129.918522 \nL 182.792651 127.79423 \nL 184.424336 125.01546 \nL 186.056021 121.755388 \nL 187.687706 117.977866 \nL 189.319391 113.655911 \nL 191.114244 108.255858 \nL 193.072266 101.596938 \nL 195.193457 93.525729 \nL 197.640984 83.248545 \nL 200.741186 69.178196 \nL 206.77842 41.57206 \nL 208.899611 33.006718 \nL 210.694464 26.675681 \nL 212.16298 22.276554 \nL 213.468328 19.044317 \nL 214.610508 16.789011 \nL 215.589519 15.307609 \nL 216.405361 14.403942 \nL 217.221204 13.808322 \nL 217.873878 13.556687 \nL 218.526552 13.5063 \nL 219.179226 13.657402 \nL 219.8319 14.009268 \nL 220.647742 14.728769 \nL 221.463585 15.753897 \nL 222.442596 17.376612 \nL 223.584775 19.788832 \nL 224.726955 22.726997 \nL 226.032303 26.675681 \nL 227.663988 32.392617 \nL 229.62201 40.191426 \nL 232.069537 50.957567 \nL 241.206973 92.210802 \nL 243.491332 101.006788 \nL 245.449354 107.731288 \nL 247.407376 113.655911 \nL 249.202229 118.379758 \nL 250.833914 122.104146 \nL 252.465599 125.314336 \nL 254.097284 128.047227 \nL 255.728969 130.345625 \nL 257.360654 132.255577 \nL 258.992339 133.824084 \nL 260.624024 135.097239 \nL 262.418878 136.208702 \nL 264.376899 137.136646 \nL 266.49809 137.875773 \nL 268.782449 138.435791 \nL 271.556313 138.879679 \nL 274.982852 139.193999 \nL 279.877907 139.396705 \nL 283.467614 139.456009 \nL 283.467614 139.456009 \n\" clip-path=\"url(#pe79e77eee6)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 145.8 \nL 43.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 294.88125 145.8 \nL 294.88125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 145.8 \nL 294.88125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 294.88125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 59.234375 \nL 148.878125 59.234375 \nQ 150.878125 59.234375 150.878125 57.234375 \nL 150.878125 14.2 \nQ 150.878125 12.2 148.878125 12.2 \nL 50.78125 12.2 \nQ 48.78125 12.2 48.78125 14.2 \nL 48.78125 57.234375 \nQ 48.78125 59.234375 50.78125 59.234375 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_28\">\n     <path d=\"M 52.78125 20.298438 \nL 62.78125 20.298438 \nL 72.78125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- mean 0,std 1 -->\n     <g transform=\"translate(80.78125 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-2c\" d=\"M 750 794 \nL 1409 794 \nL 1409 256 \nL 897 -744 \nL 494 -744 \nL 750 256 \nL 750 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(97.412109 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(158.935547 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(220.214844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(283.59375 0)\"/>\n      <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(315.380859 0)\"/>\n      <use xlink:href=\"#DejaVuSans-2c\" transform=\"translate(379.003906 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(410.791016 0)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(462.890625 0)\"/>\n      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(502.099609 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(565.576172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(597.363281 0)\"/>\n     </g>\n    </g>\n    <g id=\"line2d_29\">\n     <path d=\"M 52.78125 34.976563 \nL 62.78125 34.976563 \nL 72.78125 34.976563 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- mean 0,std 2 -->\n     <g transform=\"translate(80.78125 38.476563) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(97.412109 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(158.935547 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(220.214844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(283.59375 0)\"/>\n      <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(315.380859 0)\"/>\n      <use xlink:href=\"#DejaVuSans-2c\" transform=\"translate(379.003906 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(410.791016 0)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(462.890625 0)\"/>\n      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(502.099609 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(565.576172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(597.363281 0)\"/>\n     </g>\n    </g>\n    <g id=\"line2d_30\">\n     <path d=\"M 52.78125 49.654688 \nL 62.78125 49.654688 \nL 72.78125 49.654688 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- mean 3,std 1 -->\n     <g transform=\"translate(80.78125 53.154688) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(97.412109 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(158.935547 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(220.214844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(283.59375 0)\"/>\n      <use xlink:href=\"#DejaVuSans-33\" transform=\"translate(315.380859 0)\"/>\n      <use xlink:href=\"#DejaVuSans-2c\" transform=\"translate(379.003906 0)\"/>\n      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(410.791016 0)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(462.890625 0)\"/>\n      <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(502.099609 0)\"/>\n      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(565.576172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(597.363281 0)\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe79e77eee6\">\n   <rect x=\"43.78125\" y=\"7.2\" width=\"251.1\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "1.  **线性回归模型与噪声假设:**\n",
    "    我们考虑一个线性回归模型，其中输出 $y$ 是输入的特征向量 $\\mathbf{x}$ 的线性组合，加上一个偏置项 $b$，再加上一个噪声项 $\\epsilon$：\n",
    "    $$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon$$\n",
    "    这里的 $\\mathbf{w}$ 是权重向量，$\\mathbf{x}$ 是输入特征向量，$b$ 是偏置项。\n",
    "    假设噪声 $\\epsilon$ 服从均值为0，方差为 $\\sigma^2$ 的高斯分布，表示为：\n",
    "    $$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "2.  **似然函数（Likelihood Function）:**\n",
    "    基于上述噪声假设，我们可以推导出给定输入 $\\mathbf{x}$ 时，观测到输出 $y$ 的概率密度函数。由于 $y - (\\mathbf{w}^\\top \\mathbf{x} + b) = \\epsilon$，我们可以将高斯分布的概率密度函数（PDF）应用于这个差值。\n",
    "    对于单个数据点 $(\\mathbf{x}, y)$，其似然函数为：\n",
    "    $$P(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right)$$\n",
    "    这个公式表示了在给定的 $\\mathbf{x}$ 和模型参数 $\\mathbf{w}, b, \\sigma^2$ 下，观测到特定 $y$ 值的概率（或概率密度）。\n",
    "\n",
    "3.  **数据集的联合似然:**\n",
    "    假设我们有一组包含 $n$ 个独立同分布（i.i.d.）的数据点 $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^n$。由于数据点是独立的，整个数据集的联合似然是各个数据点似然的乘积：\n",
    "    $$P(\\mathbf{y}|\\mathbf{X}) = \\prod_{i=1}^n P(y^{(i)}|\\mathbf{x}^{(i)}) = \\prod_{i=1}^n \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2\\right) \\right)$$\n",
    "    其中 $\\mathbf{X} = \\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\}$ 和 $\\mathbf{y} = \\{y^{(1)}, \\ldots, y^{(n)}\\}$。\n",
    "\n",
    "4.  **极大似然估计（MLE）:**\n",
    "    极大似然估计的目标是找到一组参数 $\\mathbf{w}$ 和 $b$（以及 $\\sigma^2$），使得观测到当前数据集的联合似然 $P(\\mathbf{y}|\\mathbf{X})$ 最大化。\n",
    "\n",
    "5.  **对数似然与简化:**\n",
    "    直接最大化乘积形式的似然函数（尤其是包含指数项时）通常比较困难。一个常用的技巧是对似然函数取对数，因为对数函数是单调递增的，最大化 $\\log(P)$ 等价于最大化 $P$。\n",
    "    $$ \\log P(\\mathbf{y}|\\mathbf{X}) = \\sum_{i=1}^n \\left[ \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\frac{1}{2\\sigma^2}(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 \\right] $$\n",
    "    $$ \\log P(\\mathbf{y}|\\mathbf{X}) = \\sum_{i=1}^n \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\sum_{i=1}^n \\frac{1}{2\\sigma^2}(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 $$\n",
    "    $$ \\log P(\\mathbf{y}|\\mathbf{X}) = n \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 $$\n",
    "\n",
    "6.  **最小化负对数似然:**\n",
    "    在优化实践中，通常将最大化问题转化为最小化问题。最小化负对数似然 $-\\log P(\\mathbf{y}|\\mathbf{X})$ 是等价于最大化对数似然的。\n",
    "    $$ -\\log P(\\mathbf{y}|\\mathbf{X}) = -n \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 $$\n",
    "    $$ -\\log P(\\mathbf{y}|\\mathbf{X}) = n \\log(\\sqrt{2\\pi\\sigma^2}) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 $$\n",
    "    根据文本，这一项可以写为（与上述公式形式略有不同，但等价）：\n",
    "    $$ -\\log P(\\mathbf{y}|\\mathbf{X}) = \\sum_{i=1}^n \\frac{1}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 $$\n",
    "    这里 $\\sum_{i=1}^n \\frac{1}{2}\\log(2\\pi\\sigma^2) = n \\frac{1}{2}\\log(2\\pi\\sigma^2) = n\\log(\\sqrt{2\\pi\\sigma^2})$，两种写法是完全一致的。\n",
    "\n",
    "7.  **与均方误差（MSE）的联系:**\n",
    "    现在关注需要最小化的目标函数：\n",
    "    $$ \\text{Objective} = \\sum_{i=1}^n \\frac{1}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2 $$\n",
    "    *   **第一项**：$\\sum_{i=1}^n \\frac{1}{2}\\log(2\\pi\\sigma^2)$ 是一个常数项，它不依赖于待优化的参数 $\\mathbf{w}$ 和 $b$。因此，在最小化过程中，这一项可以被忽略。\n",
    "    *   **第二项**：$\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2$ 是数据集中误差（或残差）平方和的一半，再乘以一个与 $\\sigma^2$ 相关的常数 $\\frac{1}{2\\sigma^2}$。\n",
    "\n",
    "    如果我们假设 $\\sigma^2$ 是一个固定且已知（或不影响最优解）的常数，那么最小化目标函数就等价于最小化 $\\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b)^2$。这个项正是**均方误差（MSE）**的分子（不乘以 $1/n$）。因此，最小化均方误差（或均方根误差）与在高斯噪声假设下对线性模型进行极大似然估计是等价的。\n",
    "\n",
    "**总结:**\n",
    "当假设线性回归模型的误差服从高斯分布时，通过极大似然估计找到模型参数的过程，最后归结为最小化观测值与模型预测值之间的平方差之和。这就是为什么均方误差损失函数在线性回归问题中如此普遍和自然的原因。"
   ],
   "id": "272dde9a0e737a05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 练习\n",
    "好的，我们来逐一讲解这些关于线性回归的解析题。\n",
    "\n",
    "### 1. 最小化平方误差与正态分布\n",
    "\n",
    "**问题描述**: 假设我们有一些数据 $x_1, \\ldots, x_n \\in \\mathbb{R}$。我们的目标是找到一个常数 $b$，使得最小化 $\\sum_{i=1}^{n} (x_i - b)^2$。\n",
    "\n",
    "**1.1 找到最优值 $b$ 的解析解**\n",
    "\n",
    "为了找到使目标函数最小的 $b$，我们对目标函数关于 $b$ 求导，并令导数等于零。\n",
    "设损失函数为 $L(b) = \\sum_{i=1}^{n} (x_i - b)^2$。\n",
    "\n",
    "求导：\n",
    "$$\n",
    "\\frac{dL}{db} = \\frac{d}{db} \\sum_{i=1}^{n} (x_i - b)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{dL}{db} = \\sum_{i=1}^{n} \\frac{d}{db} (x_i - b)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{dL}{db} = \\sum_{i=1}^{n} 2(x_i - b)(-1)\n",
    "$$\n",
    "$$\n",
    "\\frac{dL}{db} = -2 \\sum_{i=1}^{n} (x_i - b)\n",
    "$$\n",
    "\n",
    "令导数等于零以找到极值点：\n",
    "$$\n",
    "-2 \\sum_{i=1}^{n} (x_i - b) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{i=1}^{n} (x_i - b) = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i - \\sum_{i=1}^{n} b = 0\n",
    "$$\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i - nb = 0\n",
    "$$\n",
    "$$\n",
    "nb = \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "$$\n",
    "b = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "因此，最优值 $b$ 是数据点 $x_i$ 的均值（平均值）。\n",
    "\n",
    "**1.2 这个问题及其解与正态分布有什么关系？**\n",
    "\n",
    "这个最小二乘问题与正态分布（高斯分布）有着密切的关系。如果我们假设每个数据点 $x_i$ 是由一个潜在的真实值（例如 $b$）加上服从零均值、独立同方差 $\\sigma^2$ 的正态分布噪声 $\\epsilon_i$ 产生的，即 $x_i = b + \\epsilon_i$，其中 $\\epsilon_i \\sim N(0, \\sigma^2)$。\n",
    "\n",
    "正态分布的概率密度函数 (PDF) 为：\n",
    "$$\n",
    "p(\\epsilon) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\epsilon^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "因此，对于单个数据点 $x_i$ 观测到的概率（在给定 $b$ 和 $\\sigma^2$ 的情况下）是：\n",
    "$$\n",
    "p(x_i | b, \\sigma^2) = p(\\epsilon_i = x_i - b) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - b)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "假设数据点是独立同分布的，那么观测到所有数据 $x_1, \\ldots, x_n$ 的似然函数为：\n",
    "$$\n",
    "P(x_1, \\ldots, x_n | b, \\sigma^2) = \\prod_{i=1}^{n} p(x_i | b, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - b)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$$\n",
    "P(x_1, \\ldots, x_n | b, \\sigma^2) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\sum_{i=1}^{n} \\frac{(x_i - b)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "最大似然估计 (MLE) 的目标是找到使似然函数最大的参数 $b$ 和 $\\sigma^2$。最大化似然函数等价于最大化对数似然函数：\n",
    "$$\n",
    "\\log P(x_1, \\ldots, x_n | b, \\sigma^2) = n \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\sum_{i=1}^{n} \\frac{(x_i - b)^2}{2\\sigma^2}\n",
    "$$\n",
    "$$\n",
    "\\log P(x_1, \\ldots, x_n | b, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - b)^2\n",
    "$$\n",
    "\n",
    "要最大化这个对数似然函数关于 $b$ 的值，我们只需要最小化 $\\sum_{i=1}^{n} (x_i - b)^2$ 部分（因为 $-\\frac{1}{2\\sigma^2}$ 是一个负的常数因子，而 $-\\frac{n}{2}\\log(2\\pi\\sigma^2)$ 对于 $b$ 来说是常数）。\n",
    "\n",
    "因此，最小化 $\\sum_{i=1}^{n} (x_i - b)^2$ 的问题，正是假设噪声服从正态分布时，通过最大似然估计方法求解参数 $b$ 的过程。其解 $b = \\frac{1}{n}\\sum x_i$ 是数据样本的均值。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 平方误差线性回归的解析解\n",
    "\n",
    "**问题描述**: 推导出使用平方误差的线性回归优化问题的解析解。为了简化问题，可以忽略偏置 $b$（我们可以通过向 $X$ 添加所有值为 1 的一列来做到这一点）。\n",
    "\n",
    "假设我们有一个模型 $y = Xw$，其中 $y$ 是目标向量，$X$ 是包含一个全 1 列（用于截距项）以及其他特征的矩阵，$w$ 是权重向量。\n",
    "\n",
    "**2.1 用矩阵和向量表示法写出优化问题**\n",
    "\n",
    "我们要最小化的损失函数是平方误差和 (Sum of Squared Errors, SSE)，也称为 $L_2$ 损失。\n",
    "设 $y \\in \\mathbb{R}^n$ 是目标向量，$X \\in \\mathbb{R}^{n \\times (d+1)}$ 是特征矩阵（$n$ 个样本，$d$ 个特征 + 1 个截距），$w \\in \\mathbb{R}^{d+1}$ 是权重向量。\n",
    "\n",
    "预测值为 $\\hat{y} = Xw$。\n",
    "误差向量为 $e = y - \\hat{y} = y - Xw$。\n",
    "\n",
    "损失函数 $J(w)$ 是误差向量的欧几里得范数的平方：\n",
    "$$\n",
    "J(w) = ||e||^2 = ||y - Xw||^2\n",
    "$$\n",
    "在矩阵形式下，它可以展开为：\n",
    "$$\n",
    "J(w) = (y - Xw)^T (y - Xw)\n",
    "$$\n",
    "$$\n",
    "J(w) = (y^T - w^T X^T)(y - Xw)\n",
    "$$\n",
    "$$\n",
    "J(w) = y^T y - y^T Xw - w^T X^T y + w^T X^T Xw\n",
    "$$\n",
    "由于 $y^T Xw$ 是一个标量，所以 $(y^T Xw)^T = y^T Xw$。同时，$w^T X^T y = (y^T Xw)^T$。因此，$y^T Xw = w^T X^T y$。\n",
    "$$\n",
    "J(w) = y^T y - 2 w^T X^T y + w^T X^T Xw\n",
    "$$\n",
    "(注意：在推导中，常写成 $-2 w^T X^T y$ 而不是 $-2 y^T Xw$，这是为了保持 $w$ 在中间)。\n",
    "\n",
    "**2.2 计算损失对 $w$ 的梯度**\n",
    "\n",
    "使用矩阵求导法则，我们对 $J(w)$ 关于 $w$ 求梯度：\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w} = \\frac{\\partial}{\\partial w} (y^T y) - \\frac{\\partial}{\\partial w} (2 w^T X^T y) + \\frac{\\partial}{\\partial w} (w^T X^T Xw)\n",
    "$$\n",
    "我们知道：\n",
    "*   $\\frac{\\partial}{\\partial w} (c) = 0$ （常数）\n",
    "*   $\\frac{\\partial}{\\partial w} (a^T w) = a$\n",
    "*   $\\frac{\\partial}{\\partial w} (w^T A w) = 2Aw$ （当 $A$ 对称时， $X^T X$ 是对称的）\n",
    "\n",
    "应用这些法则：\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w} = 0 - 2 (X^T y) + 2 (X^T X) w\n",
    "$$\n",
    "$$\n",
    "\\nabla_w J(w) = 2 X^T X w - 2 X^T y\n",
    "$$\n",
    "\n",
    "**2.3 通过将梯度设为 0，求解矩阵方程来找到解析解**\n",
    "\n",
    "为了找到最小化损失函数 $J(w)$ 的权重向量 $w$，我们将梯度设为零：\n",
    "$$\n",
    "2 X^T X w - 2 X^T y = 0\n",
    "$$\n",
    "$$\n",
    "X^T X w = X^T y\n",
    "$$\n",
    "为了唯一地解出 $w$，我们需要矩阵 $(X^T X)$ 是可逆的（即满秩）。在这种情况下，我们可以两边同时左乘 $(X^T X)^{-1}$：\n",
    "$$\n",
    "(X^T X)^{-1} (X^T X) w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "$$\n",
    "I w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "这就是线性回归问题的解析解，也称为**正规方程 (Normal Equation)**。\n",
    "\n",
    "**2.4 什么时候可能比使用随机梯度下降（SGD）更好？这种方法何时会失效？**\n",
    "\n",
    "**何时比 SGD 更好：**\n",
    "\n",
    "1.  **数据集较小**: 当样本数量 $n$ 和特征数量 $d$ 相对较小时，计算 $(X^T X)^{-1}$ 是可行且高效的。计算量主要在于矩阵乘法和求逆，其复杂度大致为 $O(nd^2 + d^3)$。对于小到中等规模的数据集，这通常比 SGD 需要的（可能很多次的）迭代更新更快收敛到精确解。\n",
    "2.  **无需调整超参数**: 正规方程提供了一个直接的解析解，不需要像 SGD 那样调整学习率、批量大小、迭代次数等超参数。\n",
    "3.  **保证全局最优**: 如果 $X^T X$ 可逆，正规方程直接给出了损失函数的全局最小值。SGD 是一个迭代优化过程，可能会收敛到全局最小值附近，但可能需要仔细调参才能达到很高的精度，并且可能受到局部最小值（尽管对于线性回归的 SSE 目标函数，局部最小值不存在）或鞍点的困扰（尽管在其他模型中）。\n",
    "\n",
    "**何时失效或存在问题：**\n",
    "\n",
    "1.  **数据集非常大**: 当样本量 $n$ 或特征维度 $d$ 非常大时，计算 $X^T X$（维度为 $(d+1) \\times (d+1)$）及其逆矩阵的计算成本会变得非常高昂，内存需求也很大。矩阵求逆的计算复杂度是 $O(d^3)$，这对于高维数据是难以承受的。\n",
    "2.  **特征高度相关（多重共线性）**: 如果特征之间存在高度线性相关性，矩阵 $X^T X$ 将会是奇异的（不可逆的），或者接近奇异。在这种情况下，$(X^T X)^{-1}$ 无法计算，或者计算结果非常不稳定，导致模型参数的估计有很大的不确定性。这时，正规方程就会失效。\n",
    "3.  **特征维度大于样本数量 ($d > n$)**: 在这种情况下，$X^T X$ 矩阵（$(d+1) \\times (d+1)$ 维度）的秩最多为 $n$，因此它肯定不是满秩的，$(X^T X)^{-1}$ 不存在。\n",
    "\n",
    "在现代机器学习中，尤其是处理大数据集时，SGD 及其变种（如 Adam, RMSprop）通常是更受欢迎的选择，因为它们能够处理大规模数据，并且在高维、共线性问题上通过一些正则化技术（如 L1, L2 正则化）可以更有效地工作。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 指数分布噪声下的线性回归\n",
    "\n",
    "**问题描述**: 假设控制附加噪声 $\\epsilon$ 的噪声模型是指数分布。也就是说，$p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$。\n",
    "\n",
    "**3.1 写出模型 $-\\log P(y | X)$ 下数据的负对数似然**\n",
    "\n",
    "模型是 $y_i = w^T x_i + \\epsilon_i$，其中 $\\epsilon_i$ 服从具有均值为 0 的对称指数分布（也称为双指数分布）。\n",
    "其概率密度函数 (PDF) 为：\n",
    "$$\n",
    "p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)\n",
    "$$\n",
    "所以，对于单个样本 $(x_i, y_i)$，给定 $w$ 和 $x_i$，观测到 $y_i$ 的概率密度是：\n",
    "$$\n",
    "p(y_i | x_i, w) = p(\\epsilon_i = y_i - w^T x_i) = \\frac{1}{2} \\exp\\left(-\\left|y_i - w^T x_i\\right|\\right)\n",
    "$$\n",
    "假设样本之间是独立同分布的，观测到所有数据 $(X, y)$ 的似然函数是：\n",
    "$$\n",
    "P(y | X, w) = \\prod_{i=1}^{n} p(y_i | x_i, w) = \\prod_{i=1}^{n} \\frac{1}{2} \\exp\\left(-\\left|y_i - w^T x_i\\right|\\right)\n",
    "$$\n",
    "对数似然函数是：\n",
    "$$\n",
    "\\log P(y | X, w) = \\sum_{i=1}^{n} \\log\\left(\\frac{1}{2} \\exp\\left(-\\left|y_i - w^T x_i\\right|\\right)\\right)\n",
    "$$\n",
    "$$\n",
    "\\log P(y | X, w) = \\sum_{i=1}^{n} \\left( \\log\\left(\\frac{1}{2}\\right) - \\left|y_i - w^T x_i\\right| \\right)\n",
    "$$\n",
    "$$\n",
    "\\log P(y | X, w) = n \\log\\left(\\frac{1}{2}\\right) - \\sum_{i=1}^{n} \\left|y_i - w^T x_i\\right|\n",
    "$$\n",
    "数据的负对数似然 (Negative Log-Likelihood, NLL) 是：\n",
    "$$\n",
    "-\\log P(y | X, w) = - n \\log\\left(\\frac{1}{2}\\right) + \\sum_{i=1}^{n} \\left|y_i - w^T x_i\\right|\n",
    "$$\n",
    "$$\n",
    "-\\log P(y | X, w) = n \\log(2) + \\sum_{i=1}^{n} \\left|y_i - w^T x_i\\right|\n",
    "$$\n",
    "最小化负对数似然等价于最小化 $\\sum_{i=1}^{n} |y_i - w^T x_i|$。这正是 **L1 回归**或 **最小绝对偏差 (Least Absolute Deviations, LAD)** 问题。\n",
    "\n",
    "**3.2 请试着写出解析解**\n",
    "\n",
    "要找到最小化 $L(w) = \\sum_{i=1}^{n} |y_i - w^T x_i|$ 的 $w$，我们需要考虑绝对值函数的性质。绝对值函数 $|u|$ 在 $u=0$ 处是不可导的（导数在 $u \\to 0^+$ 时为 1，在 $u \\to 0^-$ 时为 -1）。\n",
    "然而，我们可以使用**次梯度 (subgradient)** 的概念。函数 $f(u) = |u|$ 的次梯度 $g$ 满足：\n",
    "$g = \\begin{cases} 1 & \\text{if } u > 0 \\\\ -1 & \\text{if } u < 0 \\\\ \\text{any value in } [-1, 1] & \\text{if } u = 0 \\end{cases}$\n",
    "我们可以将其表示为 $g = \\text{sgn}(u)$，其中 $\\text{sgn}(u)$ 是符号函数，在 $u=0$ 处取值为 0（或者是一个定义在 $-1$ 到 $1$ 之间的值，取决于具体的优化算法）。\n",
    "\n",
    "因此，损失函数 $L(w)$ 关于 $w$ 的次梯度是：\n",
    "$$\n",
    "\\partial_w L(w) = \\sum_{i=1}^{n} \\text{sgn}(y_i - w^T x_i) x_i\n",
    "$$\n",
    "（这里我们假设 $\\text{sgn}(0)$ 的选择不影响最终最优解的确定性，或者在优化时会有特殊处理）。\n",
    "\n",
    "要找到最优解，我们将次梯度设为零向量：\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\text{sgn}(y_i - w^T x_i) x_i = 0\n",
    "$$\n",
    "这个方程与线性回归（L2 损失）的方程 $X^T X w = X^T y$ 不同。因为 $\\text{sgn}$ 函数依赖于 $w$ 的值（通过 $y_i - w^T x_i$ 的符号），这个方程通常不像 L2 回归那样可以容易地通过矩阵求逆得到一个封闭形式的解析解 $w = (X^T X)^{-1} X^T y$。\n",
    "\n",
    "**结论**: L1 回归（最小绝对偏差）通常没有简单的、直接的解析解，需要依赖迭代优化算法来求解。\n",
    "\n",
    "**3.3 提出一种随机梯度下降（SGD）算法来解决这个问题。哪里可能出错？（提示：当我们不断更新参数时，在驻点附近会发生什么情况）**\n",
    "\n",
    "**SGD 算法**:\n",
    "\n",
    "1.  **初始化**: 随机初始化权重向量 $w$（例如，所有元素置零或从小的随机值开始）。\n",
    "2.  **学习率**: 选择一个学习率 $\\alpha > 0$。通常需要一个学习率衰减策略（例如，$\\alpha_t = \\frac{\\alpha_0}{1+kt}$）。\n",
    "3.  **迭代更新**:\n",
    "    *   在一个 epoch 内，可以遍历所有样本 $(x_i, y_i)$。\n",
    "    *   对于每个样本 $i$：\n",
    "        *   计算预测值：$\\hat{y}_i = w^T x_i$。\n",
    "        *   计算误差：$err_i = y_i - \\hat{y}_i$。\n",
    "        *   计算 L1 损失的子梯度：\n",
    "            *   如果 $err_i > 0$，则 $\\text{sgn}(err_i) = 1$。\n",
    "            *   如果 $err_i < 0$，则 $\\text{sgn}(err_i) = -1$。\n",
    "            *   如果 $err_i = 0$，通常选择 $\\text{sgn}(0) = 0$ 来避免更新，或者随机选择一个 [-1, 1] 之间的值，或者也设为 0（这取决于具体的实现和算法变种）。这里我们选取 $\\text{sgn}(0) = 0$ 进行讨论。\n",
    "        *   计算样本的次梯度：$\\partial_i L(w) = \\text{sgn}(err_i) x_i$。\n",
    "        *   更新权重：$w \\leftarrow w - \\alpha \\cdot \\text{sgn}(err_i) x_i$。\n",
    "\n",
    "**可能出错的地方（提示：驻点附近的情况）**:\n",
    "\n",
    "1.  **零点的不可导性与震荡**:\n",
    "    *   **驻点**: 所谓的“驻点”对于 L1 损失是次梯度的零点。这意味着 $\\sum_{i=1}^{n} \\text{sgn}(y_i - w^T x_i) x_i = 0$。\n",
    "    *   **问题**: L1 损失的梯度（或次梯度）是阶跃函数。当 $y_i - w^T x_i$ 接近于 0 时，$\\text{sgn}(y_i - w^T x_i)$ 的值会从 -1 突然跳到 1（或者相反）。\n",
    "    *   **在驻点附近**: 如果当前 $w$ 使得 $y_i - w^T x_i$ 对于某些样本非常接近 0，那么在 SGD 更新过程中，这些样本的梯度可能会在 $x_i$ 和 $-x_i$ 之间剧烈切换。\n",
    "    *   **情况**: 如果一个样本的 $y_i - w^T x_i$ 变为 0，而我们选择 $\\text{sgn}(0)=0$，那么该样本不再更新 $w$。但如果之后 $w$ 的微小变化使得 $y_i - w^T x_i$ 变为一个非常小但不为零的值，其符号又会突然改变。\n",
    "    *   **结果**: 即使是在全局最小值附近，SGD 也可能因为这些梯度跳变而导致参数 $w$ 围绕最优解发生**震荡 (oscillation)**，而不是平滑地收敛到精确值。如果学习率 $\\alpha$ 足够小，并且是衰减的，震荡的幅度可能会减小，但可能永远不会完全停止。\n",
    "    *   **收敛的定义**: 对于 L1 回归，收敛可能意味着参数进入一个使次梯度包含零向量的“区域”，而不是精确地落在某一点。\n",
    "\n",
    "2.  **学习率的选择**:\n",
    "    *   如果学习率过大，即使在非驻点区域，也可能导致参数在最优值附近大幅度跳过，难以收敛。\n",
    "    *   如果学习率过小，收敛过程会非常缓慢。\n",
    "\n",
    "3.  **样本顺序的影响**:\n",
    "    *   SGD 的更新路径对样本的顺序非常敏感。不同的样本顺序可能导致不同的收敛行为和最终的解。\n",
    "\n",
    "4.  **批次大小**:\n",
    "    *   纯粹的 SGD（批量大小为 1）对噪声最敏感。使用小批量 SGD（Mini-batch SGD）可以平滑梯度，减少震荡，提高稳定性，但会增加计算复杂度（每个更新步骤需要处理一个批次）。\n",
    "\n",
    "**总结**: L1 回归的 SGD 算法在处理不可导点时需要特别小心。即使成功收敛，也可能在最优解附近出现震荡，这与 L2 回归的平滑收敛行为有所不同。关键在于如何有效地处理 $\\text{sgn}(0)$ 的情况以及如何选择合适的学习率衰减策略来减小震荡并最终找到一个接近最优的解。"
   ],
   "id": "aefee86c269d438d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2d47d4c298df475"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
