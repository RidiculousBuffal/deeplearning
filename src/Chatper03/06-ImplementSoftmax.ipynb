{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 初始化模型参数",
   "id": "8e3c13a4c9c50107"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.437018Z",
     "start_time": "2025-08-06T14:31:56.432961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "if is_cuda_available:\n",
    "    print(\"CUDA is available! PyTorch is using the GPU.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU index: {torch.cuda.current_device()}\")\n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    d2l.try_gpu(0)\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is using the CPU.\")"
   ],
   "id": "75ac2fb8bb7f4f98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! PyTorch is using the GPU.\n",
      "Number of GPUs available: 1\n",
      "Current GPU index: 0\n",
      "Current GPU name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.570208Z",
     "start_time": "2025-08-06T14:31:56.536728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from d2l.torch import get_dataloader_workers, Animator\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../../data', train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../../data', train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=get_dataloader_workers()))\n",
    "\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ],
   "id": "6a1018a10a287838",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.592225Z",
     "start_time": "2025-08-06T14:31:56.588227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10"
   ],
   "id": "e6b66481e3373c3b",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.  **线性模型**：对于每个输入样本 $\\mathbf{x}$，模型会计算出它属于每个类别 $k$ 的一个“分数”（或称为 logit），记为 $o_k$。这个计算过程是线性的：\n",
    "    $$\n",
    "    \\mathbf{o} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n",
    "    $$\n",
    "    其中：\n",
    "    *   $\\mathbf{X}$ 是输入特征矩阵，每一行是一个样本。\n",
    "    *   $\\mathbf{W}$ 是权重矩阵。\n",
    "    *   $\\mathbf{b}$ 是偏置向量。\n",
    "    *   $\\mathbf{o}$ 是输出的 logits 矩阵，每一行代表一个样本对应所有类别的分数。"
   ],
   "id": "e07639d453db95"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.604954Z",
     "start_time": "2025-08-06T14:31:56.600875Z"
    }
   },
   "source": [
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 定义softmax\n",
    "操作2.  **Softmax 函数**：得到这些分数后，直接比较大小是不够的，我们希望得到一个概率分布。Softmax 函数可以将这些任意实数的分数转换成一个概率分布，其中每个类别的概率值都在 (0, 1) 之间，且所有类别的概率之和为 1。对于单个样本的输出向量 $\\mathbf{o} = (o_1, o_2, ..., o_K)$，其 Softmax 变换后的概率向量 $\\hat{\\mathbf{y}}$ 中第 $i$ 个元素为：\n",
    "    $$\n",
    "    \\hat{y}_i = \\text{softmax}(\\mathbf{o})_i = \\frac{\\exp(o_i)}{\\sum_{j=1}^{K} \\exp(o_j)}\n",
    "    $$\n",
    "    其中 $K$ 是类别的总数。"
   ],
   "id": "ccdc015122baa3eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.625441Z",
     "start_time": "2025-08-06T14:31:56.622472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition"
   ],
   "id": "292fb32c73f887c4",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "dbab58110fd0260e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.653737Z",
     "start_time": "2025-08-06T14:31:56.650331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b)"
   ],
   "id": "924a3a30aef03a7b",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义损失函数",
   "id": "789b750c0e50c692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3.  **损失函数 (Cross-Entropy Loss)**：为了衡量模型预测的好坏，我们使用交叉熵损失函数。对于单个样本，其真实标签通常用 one-hot 向量 $\\mathbf{y}$ 表示（例如，如果真实类别是第 $c$ 类，则 $y_c=1$，其余 $y_j=0$）。损失函数定义为：\n",
    "    $$\n",
    "    L(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^{K} y_j \\log(\\hat{y}_j)\n",
    "    $$\n",
    "    因为 $\\mathbf{y}$ 是 one-hot 向量，这个求和式可以简化为：\n",
    "    $$\n",
    "    L = -\\log(\\hat{y}_c)\n",
    "    $$\n",
    "    其中 $c$ 是该样本的真实类别索引。我们的目标就是最小化这个损失。\n"
   ],
   "id": "312c61dabd8ac03f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.737178Z",
     "start_time": "2025-08-06T14:31:56.733373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])"
   ],
   "id": "3262a29c742fa5d4",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 分类精度",
   "id": "ba9f609d60eee0fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.761703Z",
     "start_time": "2025-08-06T14:31:56.757706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        \"\"\"\n",
    "        如果 `y_hat` 的前两行是：`tensor([[0.1, 0.2, 0.7],  # 样本0，预测为类别2``[0.8, 0.1, 0.1]]) # 样本1，预测为类别0`经过 `y_hat.argmax(axis=1)` 后，`y_hat` 会变成：`tensor([2, 0])`\n",
    "        \"\"\"\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    \"\"\"\n",
    "    如果 `y_hat` (预测) 是 `tensor([2, 0])`，而 `y` (真实) 是 `tensor([2, 1])`。那么 `cmp` 的结果就是 `tensor([True, False])`。\n",
    "    \"\"\"\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    \"\"\"\n",
    "    接上一步，`cmp` 是 `tensor([True, False])`。\n",
    "    1.  `cmp.type(y.dtype)` 将其转换为 `tensor([1, 0])`。\n",
    "    2.  `.sum()` 对 `tensor([1, 0])` 求和，得到 `tensor(1)`。\n",
    "    3.  `float()` 将其转换为 `1.0`。\n",
    "    \"\"\"\n",
    "    return float(cmp.type(y.dtype).sum())\n"
   ],
   "id": "7c581a4c297a32ce",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "1.  **`net.eval()`**: 这是一个非常重要的步骤。当我们训练模型时，会使用 `net.train()` 模式，这会启用一些特定的层，比如 `Dropout`（随机丢弃神经元）和 `BatchNormalization`（使用当前批次的均值和方差）。但在评估和预测时，我们希望模型的行为是确定性的，不希望有随机性。`net.eval()` 会关闭 `Dropout`，并让 `BatchNormalization` 使用在整个训练集上学习到的全局均值和方差，从而保证评估结果的稳定和可复现。\n",
    "\n",
    "2.  **`metric = Accumulator(2)`**: 如上所述，这里创建了一个累加器，准备好两个位置。\n",
    "    *   `metric[0]` 将用于累加 **总的正确预测数**。\n",
    "    *   `metric[1]` 将用于累加 **总的样本数**。\n",
    "\n",
    "3.  **`for X, y in data_iter:`**: 这个循环会遍历 `data_iter`（比如 `test_iter`）中的所有数据，一次一个批次 (batch)。\n",
    "\n",
    "4.  **`metric.add(accuracy(net(X), y), y.numel())`**: 这是整个函数的心脏。对于当前的批次 `(X, y)`：\n",
    "    *   `net(X)`: 模型对输入 `X` 进行预测，得到概率矩阵 `y_hat`。\n",
    "    *   `accuracy(net(X), y)`: 调用我们之前分析过的 `accuracy` 函数，它会返回**当前这个批次中预测正确的样本数量**（一个整数）。\n",
    "    *   `y.numel()`: `y` 是当前批次的真实标签张量，`.numel()` 方法返回该张量中元素的总数，也就是**当前批次的大小**（batch size）。\n",
    "    *   `metric.add(...)`: 将这两个计算出的数值——“本批次正确数”和“本批次样本数”——传递给 `add` 方法，累加到 `metric` 中。\n",
    "\n",
    "5.  **`return metric[0] / metric[1]`**: 当 `for` 循环结束时，`metric` 已经遍历了整个数据集。\n",
    "    *   `metric[0]` 存储了所有批次正确数量的总和，即 $ \\text{Total Correct Predictions} $。\n",
    "    *   `metric[1]` 存储了所有批次样本数量的总和，即 $ \\text{Total Number of Samples} $。\n",
    "    *   两者相除，正好就是我们定义的分类精度公式：\n",
    "      $$\n",
    "      \\text{Accuracy} = \\frac{\\sum_{\\text{all batches}} \\text{correct\\_in\\_batch}}{\\sum_{\\text{all batches}} \\text{samples\\_in\\_batch}} = \\frac{\\text{Total Correct Predictions}}{\\text{Total Number of Samples}}\n",
    "      $$\n"
   ],
   "id": "46377f72437ab386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.825290Z",
     "start_time": "2025-08-06T14:31:56.820298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ],
   "id": "62a1a4bcc21c2a6d",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.880278Z",
     "start_time": "2025-08-06T14:31:56.876191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Accumulator:  # @save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        # 初始化一个长度为 n 的列表，所有元素都为 0.0\n",
    "        # 这 n 个位置将分别用于存储 n 个需要累加的变量\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        # 使用 *args 接收任意数量的参数\n",
    "        # zip会将 self.data 和 args 按位置配对\n",
    "        # 例如 self.data=[0,0], args=(10, 256) -> zip后为 [(0,10), (0,256)]\n",
    "        # 然后列表推导式会逐对相加, 更新 self.data\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        # 将所有累加的变量重置为 0\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 这是一个 \"魔术方法\", 让我们能像访问列表一样访问累加器的数据\n",
    "        # 例如,可以直接用 metric[0] 来获取第一个累加的变量\n",
    "        return self.data[idx]"
   ],
   "id": "8a4ea2f53599d19c",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "```python\n",
    "if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "else:\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "```\n",
    "\n",
    "*   这部分代码通过一个 `if/else` 结构，优雅地兼容了两种不同的优化方式。\n",
    "\n",
    "*   **分支一: 使用标准的 PyTorch 优化器 (`torch.optim.Optimizer`)**\n",
    "    *   `updater.zero_grad()`: **梯度清零**。由于 PyTorch 的梯度是累加的，所以在每次计算新一批数据的梯度前，必须将之前存储的梯度清零。\n",
    "    *   `l.mean().backward()`: **反向传播 (Backpropagation)**。这里先对批次中所有样本的损失 `l` 取平均值 (`.mean()`)，得到一个标量。然后调用 `.backward()`，PyTorch 的自动求导引擎会根据这个标量损失计算出模型中所有参数（`W` 和 `b`）的梯度。使用 `.mean()` 是标准做法，它使得学习率的选择与批次大小（batch size）无关。\n",
    "    *   `updater.step()`: **更新参数**。优化器根据计算出的梯度和预设的学习率，自动更新模型的所有参数。例如，对于随机梯度下降(SGD)，这一步执行的就是 $ \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\nabla_{\\mathbf{W}} L $。\n",
    "\n",
    "*   **分支二: 使用我们从零开始实现的自定义 `updater` 函数**\n",
    "    *   `l.sum().backward()`: **反向传播**。这里对损失 `l` 进行求和 (`.sum()`)。这样做得到的梯度是整个批次损失之和的梯度，而不是平均损失的梯度。\n",
    "    *   `updater(X.shape[0])`: **更新参数**。调用我们自己写的 `updater` 函数。这个自定义函数需要手动实现参数更新的逻辑。它接收批次大小 `X.shape[0]` 作为参数。为什么需要这个参数？因为我们的梯度是基于损失之和计算的，所以在手动更新时，通常需要除以批次大小来得到平均梯度，即 $ \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{\\nabla_{\\mathbf{W}} L_{sum}}{\\text{batch\\_size}} $。将 `batch_size` 传进去就是为了完成这个除法。"
   ],
   "id": "ac8849e638aa3afb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.926601Z",
     "start_time": "2025-08-06T14:31:56.921289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    \"\"\"\n",
    "    net: 你要训练的神经网络模型。它可以是 PyTorch 内置的 nn.Module，也可以是我们之前从零开始定义的那个 net 函数。\n",
    "    train_iter: 训练数据的迭代器，比如我们用 DataLoader 创建的 train_iter。它会逐批次地提供训练数据 (X, y)。\n",
    "    loss: 损失函数，比如我们之前定义的 cross_entropy 函数，或者 PyTorch 的 nn.CrossEntropyLoss。它用来衡量模型预测的好坏。\n",
    "    updater: 更新器。这是本函数的一个非常巧妙的设计，它既可以是一个标准的 PyTorch 优化器（如 torch.optim.SGD），也可以是一个我们自定义的更新函数\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    metric = Accumulator(3)\n",
    "    \"\"\"\n",
    "    metric[0]: 用来累加所有批次的总损失。\n",
    "    metric[1]: 用来累加所有批次的总正确预测数。\n",
    "    metric[2]: 用来累加所有批次的总样本数。\n",
    "    \"\"\"\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(l.sum(), accuracy(y_hat, y), y.numel())\n",
    "        \"\"\"\n",
    "        在完成参数更新后，我们记录下这个批次的训练情况。\n",
    "        l.sum(): 当前批次的总损失。\n",
    "        accuracy(y_hat, y): 当前批次预测正确的样本数。\n",
    "        y.numel(): 当前批次的总样本数。\n",
    "        metric.add(...): 将这三个值分别累加到 metric 的三个槽位中。\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        metric[0] / metric[2]: 平均训练损失 (Average Training Loss)。\n",
    "        metric[1] / metric[2]: 平均训练精度 (Average Training Accuracy)。\n",
    "        \"\"\"\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ],
   "id": "82344e89cd37c8eb",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**分步解析**:\n",
    "\n",
    "1.  **`animator = Animator(...)`**: 在训练开始前，首先初始化一个 `Animator` 对象。\n",
    "    *   `xlabel='epoch'`: 设置图表的 x 轴标签为 'epoch'。\n",
    "    *   `xlim=[1, num_epochs]`: 设置 x 轴的显示范围从 1 到 `num_epochs`。\n",
    "    *   `ylim=[0.3, 0.9]`: 设置 y 轴的显示范围。这通常是根据经验预设的一个合理范围，用于观察损失和精度的变化。\n",
    "    *   `legend=['train loss', 'train acc', 'test acc']`: 这是**非常关键**的一步。它定义了图表中将要绘制的三条曲线的名称。这三条线将分别对应：训练损失、训练精度和测试精度。\n",
    "\n",
    "2.  **`for epoch in range(num_epochs):`**: 这是训练的主循环，它会迭代 `num_epochs` 次，每一次迭代代表一个完整的训练周期。\n",
    "\n",
    "3.  **`train_metrics = train_epoch_ch3(...)`**: 在循环内部，首先调用 `train_epoch_ch3` 函数。我们已经知道，这个函数会用全部训练数据对模型进行一次训练，并返回一个包含两个元素的元组 (tuple)：`(平均训练损失, 平均训练精度)`。这个元组被赋值给 `train_metrics`。\n",
    "\n",
    "4.  **`test_acc = evaluate_accuracy(...)`**: 在一个训练周期结束后，我们**立刻**在测试集 `test_iter` 上评估模型的性能。这至关重要，因为测试集精度（`test_acc`）是衡量模型**泛化能力**的核心指标。它可以帮助我们判断模型是否出现了过拟合（即在训练集上表现很好，但在未见过的数据上表现很差）。\n",
    "\n",
    "5.  **`animator.add(epoch + 1, train_metrics + (test_acc,))`**: 这是将当前周期的结果绘制到图表上的核心步骤。\n",
    "    *   `epoch + 1`: 这是 x 轴的坐标，代表当前的周期数（从 1 开始）。\n",
    "    *   `train_metrics + (test_acc,)`: 这是 y 轴的坐标。这里有一个巧妙的元组拼接操作：\n",
    "        *   `train_metrics` 是 `(train_loss, train_acc)`。\n",
    "        *   `(test_acc,)` 是一个只包含一个元素的元组。\n",
    "        *   两者相加得到一个新的元组：`(train_loss, train_acc, test_acc)`。\n",
    "    *   这个新元组中的三个值，恰好按顺序对应了我们在第 1 步中 `legend` 定义的三条曲线。`Animator` 会将这三个值分别添加到对应的曲线上，并更新图表。\n",
    "\n",
    "6.  **`train_loss, train_acc = train_metrics`**: 循环结束后，`train_metrics` 中保存的是**最后一个** epoch 的训练损失和训练精度。这里将其解包到两个独立的变量中。\n",
    "\n",
    "7.  **`assert ...`**: 断言语句。这是一种自动化的检查机制，用于验证训练结果是否达到了预期的基本标准。如果 `assert` 后面的条件为 `False`，程序会立即报错并显示相关信息。\n",
    "    *   `assert train_loss < 0.5`: 检查最终的训练损失是否足够低。\n",
    "    *   `assert train_acc > 0.7`: 检查最终的训练精度是否达到了一个可接受的水平。\n",
    "    *   `assert test_acc > 0.7`: 检查最终的测试精度是否也达到了可接受的水平。\n",
    "    *   这些断言是保证代码和模型按预期工作的良好实践。"
   ],
   "id": "2768c09935043b5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.953846Z",
     "start_time": "2025-08-06T14:31:56.948843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ],
   "id": "d4795d05788fbf10",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:31:56.981203Z",
     "start_time": "2025-08-06T14:31:56.977209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr=0.1\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([W,b],lr,batch_size)"
   ],
   "id": "ab6e731c7a63e2c1",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs=10\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ],
   "id": "c8f9fee1ef6c776d",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T14:37:53.210284Z",
     "start_time": "2025-08-06T14:37:48.622290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_ch3(net,test_iter,n=6):\n",
    "    for X,y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))\n",
    "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(X[0:n].reshape(n,28,28),1,n,titles=titles[0:n])\n",
    "\n",
    "predict_ch3(net,test_iter)\n",
    "    "
   ],
   "id": "de71a94c69e31365",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x150 with 6 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"516.6pt\" height=\"114.698357pt\" viewBox=\"0 0 516.6 114.698357\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-08-06T22:37:53.162776</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 114.698357 \nL 516.6 114.698357 \nL 516.6 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 7.2 107.498357 \nL 78.942857 107.498357 \nL 78.942857 35.7555 \nL 7.2 35.7555 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p500973106d)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image1ec81ab963\" transform=\"scale(1 -1) translate(0 -72)\" x=\"7.2\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 7.2 107.498357 \nL 7.2 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 78.942857 107.498357 \nL 78.942857 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 7.2 107.498357 \nL 78.942857 107.498357 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 7.2 35.7555 \nL 78.942857 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_1\">\n    <!-- ankle boot -->\n    <g transform=\"translate(11.348304 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \nL 1159 4863 \nL 1159 1991 \nL 2875 3500 \nL 3609 3500 \nL 1753 1863 \nL 3688 0 \nL 2938 0 \nL 1159 1709 \nL 1159 0 \nL 581 0 \nL 581 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-61\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(61.279297 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6b\" transform=\"translate(124.658203 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(182.568359 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(210.351562 0)\"/>\n     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(271.875 0)\"/>\n     <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(303.662109 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(367.138672 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(428.320312 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(489.501953 0)\"/>\n    </g>\n    <!-- ankle boot -->\n    <g transform=\"translate(11.348304 29.7555) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-61\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(61.279297 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6b\" transform=\"translate(124.658203 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(182.568359 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(210.351562 0)\"/>\n     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(271.875 0)\"/>\n     <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(303.662109 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(367.138672 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(428.320312 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(489.501953 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 93.291429 107.498357 \nL 165.034286 107.498357 \nL 165.034286 35.7555 \nL 93.291429 35.7555 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pcddfc1e6f5)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image36a5ed1d60\" transform=\"scale(1 -1) translate(0 -72)\" x=\"93.291429\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 93.291429 107.498357 \nL 93.291429 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 165.034286 107.498357 \nL 165.034286 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 93.291429 107.498357 \nL 165.034286 107.498357 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 93.291429 35.7555 \nL 165.034286 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_2\">\n    <!-- pullover -->\n    <g transform=\"translate(104.836607 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(63.476562 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(126.855469 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(154.638672 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(182.421875 0)\"/>\n     <use xlink:href=\"#DejaVuSans-76\" transform=\"translate(243.603516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(302.783203 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(364.306641 0)\"/>\n    </g>\n    <!-- pullover -->\n    <g transform=\"translate(104.836607 29.7555) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-70\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(63.476562 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(126.855469 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(154.638672 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(182.421875 0)\"/>\n     <use xlink:href=\"#DejaVuSans-76\" transform=\"translate(243.603516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(302.783203 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(364.306641 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 179.382857 107.498357 \nL 251.125714 107.498357 \nL 251.125714 35.7555 \nL 179.382857 35.7555 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p06b2f08aec)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image478ae731b8\" transform=\"scale(1 -1) translate(0 -72)\" x=\"179.382857\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 179.382857 107.498357 \nL 179.382857 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 251.125714 107.498357 \nL 251.125714 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 179.382857 107.498357 \nL 251.125714 107.498357 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 179.382857 35.7555 \nL 251.125714 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_3\">\n    <!-- trouser -->\n    <g transform=\"translate(193.812723 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(193.812723 29.7555) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 265.474286 107.498357 \nL 337.217143 107.498357 \nL 337.217143 35.7555 \nL 265.474286 35.7555 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#p55b28a3868)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image9be62038c8\" transform=\"scale(1 -1) translate(0 -72)\" x=\"265.474286\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 265.474286 107.498357 \nL 265.474286 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 337.217143 107.498357 \nL 337.217143 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 265.474286 107.498357 \nL 337.217143 107.498357 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 265.474286 35.7555 \nL 337.217143 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_4\">\n    <!-- trouser -->\n    <g transform=\"translate(279.904152 16.318125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(279.904152 29.7555) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 351.565714 107.498357 \nL 423.308571 107.498357 \nL 423.308571 35.7555 \nL 351.565714 35.7555 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pca7d4fe9ae)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image12dcba14b9\" transform=\"scale(1 -1) translate(0 -72)\" x=\"351.565714\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 351.565714 107.498357 \nL 351.565714 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 423.308571 107.498357 \nL 423.308571 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 351.565714 107.498357 \nL 423.308571 107.498357 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 351.565714 35.7555 \nL 423.308571 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- shirt -->\n    <g transform=\"translate(374.023393 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-73\"/>\n     <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(52.099609 0)\"/>\n     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(115.478516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(143.261719 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(184.375 0)\"/>\n    </g>\n    <!-- shirt -->\n    <g transform=\"translate(374.023393 29.7555) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-73\"/>\n     <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(52.099609 0)\"/>\n     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(115.478516 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(143.261719 0)\"/>\n     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(184.375 0)\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 437.657143 107.498357 \nL 509.4 107.498357 \nL 509.4 35.7555 \nL 437.657143 35.7555 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g clip-path=\"url(#pd91bfc9948)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"image0b0c028540\" transform=\"scale(1 -1) translate(0 -72)\" x=\"437.657143\" y=\"-35.498357\" width=\"72\" height=\"72\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 437.657143 107.498357 \nL 437.657143 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 509.4 107.498357 \nL 509.4 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 437.657143 107.498357 \nL 509.4 107.498357 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 437.657143 35.7555 \nL 509.4 35.7555 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_6\">\n    <!-- trouser -->\n    <g transform=\"translate(452.087009 16.318125) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n    <!-- trouser -->\n    <g transform=\"translate(452.087009 29.7555) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(78.072266 0)\"/>\n     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(139.253906 0)\"/>\n     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(202.632812 0)\"/>\n     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(254.732422 0)\"/>\n     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(316.255859 0)\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p500973106d\">\n   <rect x=\"7.2\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n  </clipPath>\n  <clipPath id=\"pcddfc1e6f5\">\n   <rect x=\"93.291429\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n  </clipPath>\n  <clipPath id=\"p06b2f08aec\">\n   <rect x=\"179.382857\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n  </clipPath>\n  <clipPath id=\"p55b28a3868\">\n   <rect x=\"265.474286\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n  </clipPath>\n  <clipPath id=\"pca7d4fe9ae\">\n   <rect x=\"351.565714\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n  </clipPath>\n  <clipPath id=\"pd91bfc9948\">\n   <rect x=\"437.657143\" y=\"35.7555\" width=\"71.742857\" height=\"71.742857\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f2a1964149308bc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
